{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A one-stop hub for all your DevOps documentation needs, containing helpful guides, how-to tutorials, and top-notch best practices.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Monitoring</li> </ul> <p>Your go-to source for DevOps monitoring expertise, offering insights, tools, and techniques to keep a vigilant eye on your systems and applications.</p> <ul> <li>Logging</li> </ul> <p>Unlock the power of DevOps logging with our repository, packed with insights, tutorials, and solutions to help you capture, analyze, and make sense of your application logs    </p>"},{"location":"Logging/kafka/","title":"KAFKA","text":"<p>Introduction</p> <p>Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.</p> <p>Why Kafka? Benefits and Use Cases</p> <ul> <li>Data Integration</li> <li>Metrics and Monitoring</li> <li>Log Aggregation</li> <li>Stream Processing</li> <li>Publish-Subscribe Messaging</li> <li>Kafka Architecture \u2013 Fundamental Concepts</li> <li>Kafka Topics</li> <li>Kafka Partitioning</li> </ul> <p>How Kafka Partitioning Works</p> <p></p> <ul> <li>Kafka Brokers      Kafka brokers are like computer workers in the Kafka system. They store and handle events, manage data partitions, and work together for reliability. Think of them as the backbone of Kafka.     So, brokers are the worker bees of the Kafka system, managing events and making sure everything runs smoothly.</li> </ul> <p>Replications</p> <pre><code>- To ensure data safety, Kafka creates copies of data partitions called leader and follower replicas. When you write data, it goes to the leader, and then both leader and followers automatically replicate the data. If a node fails, another takes over. As a developer, you don't need to worry much about this process. Just know your data is secure and resilient.\nClient Applications \n- In Kafka, there are two main types of client applications: producers and consumers. Producers put messages into topics, while consumers read messages from topics. These are the building blocks for working with Kafka. Everything that's not a Kafka broker is essentially a producer or consumer, or both. They're how you connect and work with a Kafka cluster       \nKafka Producers\n- The API surface of the producer library is fairly lightweight: In Java, there is a class called KafkaProducer that you use to connect to the cluster. \nKafka Consumers\n- Using the consumer API is similar in principle to the producer. You use a class called KafkaConsumer to connect to the cluster (passing a configuration map to specify the address of the cluster, security, and other parameters).\n</code></pre> <p>Kafka Installation</p> <pre><code>Prerequisite: Java 11 or 17 should be installed on the system\nhttps://kafka.apache.org/documentation.html#java\n</code></pre> <p></p> <p>Download Apache Kafka</p> <pre><code>https://kafka.apache.org/downloads\n</code></pre> <ul> <li> <p>Extract or untar the downloaded kafka file <pre><code>tar xzvf kafka_2.12-3.5.1.tgz\n</code></pre> </p> </li> <li> <p>Now test Kafka and move the the kafka directory and use the below command    </p> </li> </ul> <pre><code>bin/kafka-topic.sh\n</code></pre> <p></p> <ul> <li>Setting the Path </li> </ul> <pre><code>Open '.bashrc' file using the command: \nvi .bashrc\n</code></pre> <p></p> <ul> <li>Move to the end and set the path using the command</li> </ul> <pre><code>export PATH=/home/rajivgs/Downloads/webi/kafka_2.12-3.5.1/bin:$PATH\n</code></pre> <p></p> <ul> <li>To test, run the command from and directory.</li> </ul> <pre><code>kafka-topic.sh\n</code></pre> <p></p> <p>If the below output is shown, it means the path is successfully set. If not, something is wrong.</p> <ul> <li>Starting Zookeeper Server on Linux</li> </ul> <pre><code>To start zookeeper, there are following below steps used\n\nStep 1: Move to the and create a new directory 'data'\n\nStep 2: Again, move to the data directory, and make two new directories as 'zookeeper' and 'kafka'.\n</code></pre> <p></p> <p>To run the zookeeper server. Open the zookeeper.properties file, which is located under the config folder.  <pre><code> vi config/zookeeper.properties\n</code></pre></p> <p>Edit the value of dataDIr by placing the path of the newly created zookeeper folder. <pre><code> dataDir=/home/rajivgs/Downloads/webi/kafka_2.12-3.5.1/data/zookeeper\n\n zookeeper-server-start.sh config/zookeeper.properties \n</code></pre></p> <p></p> <p><pre><code>zookeeper-server-start.sh config/zookeeper.properties\n</code></pre> </p> <p>Move to the 'config' folder, and open server.properties file. <pre><code>vi config/server.properties\n</code></pre></p> <pre><code>Edit the value of logs.dir=/path/data/kafka as shown below\n</code></pre> <p></p> <p>Now, run the Kafka server by using the following command: <pre><code>  kafka-server-start.sh config/server.properties\n</code></pre></p> <p></p> <p>Creating Kafka Topics Initially, make sure that both zookeeper, as well as the Kafka server, should be started.</p> <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topic-name  ||\nkafka-topics.sh --bootstrap-server localhost:9092 --create --topic logger --partitions 1 --replication-factor 1\n</code></pre> <p></p> <p>Here's a breakdown of the command in short points:</p> <ul> <li> <p><code>kafka-topics.sh</code>: This is the Kafka command-line tool used to manage Kafka topics.</p> </li> <li> <p><code>--bootstrap-server localhost:9092</code>: Specifies the Kafka broker(s) to connect to. In this case, it's connecting to a Kafka broker running locally on port 9092.</p> </li> <li> <p><code>--create</code>: Indicates that you want to create a new Kafka topic.</p> </li> <li> <p><code>--topic logger</code>: Specifies the name of the topic to be created, in this case, the topic name is \"logger\".</p> </li> <li> <p><code>--partitions 1</code>: Specifies the number of partitions for the topic. Partitions are the units of parallelism in Kafka. Here, you're creating the topic with 1 partition.</p> </li> <li> <p><code>--replication-factor 1</code>: Sets the replication factor for the topic. Replication ensures data durability and availability. Here, you're using a replication factor of 1, meaning there's only one copy of each partition's data.</p> </li> </ul> <p>So, the command is creating a Kafka topic named \"logger\" with a single partition and a replication factor of 1, connecting to a Kafka broker running on localhost at port 9092.</p> <p>Listing the number of Topics <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 --list\n</code></pre></p> <ul> <li> <p>Describing the topic <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 -describe --topic topic-name \nkafka-topics.sh --bootstrap-server kafka2.rajivgopalsingh.com.np:9092 -describe --topic logger-01 \n</code></pre></p> </li> <li> <p>Deleting the topic <pre><code>kafka-topics.sh --bootstrap-server localhost:9092  --topic topic-name  -delete \nkafka-topics.sh --bootstrap-server kafka2.rajivgopalsingh.com.np:9092  --topic logger-01  -delete \n</code></pre></p> </li> <li> <p>Kafka Console Producer <pre><code>kafka-console-producer.sh --broker-list localhost:9092 --topic topic-name\n</code></pre></p> </li> <li> <p>Kafka Console Consumer <pre><code>kafka-console-consumerr.sh --broker-list localhost:9092 --topic topic-name  ||\nkafka-console-producer.sh --broker-list localhost:9092 --topic topic-name --from-beginning \n</code></pre></p> </li> </ul> <p>Practice:</p> <pre><code>1. https://kube-logging.dev/docs/examples/kafka-nginx/\n</code></pre>"},{"location":"Monitoring/1.%20setup_prometheus-grafana/","title":"Setup the Prometheus and Grafana","text":""},{"location":"Monitoring/1.%20setup_prometheus-grafana/#prometheus-setup","title":"Prometheus Setup","text":"<ul> <li>Prometheus Installation <pre><code> helm repo add prometheus-community https://prometheus-community.github.io/helm-charts            \n helm repo update                                                                                 \n helm install name prometheus-community/prometheus --namespace namepsace-name                     \n</code></pre></li> <li>Check the status of the pods. Check the promethues server by doing the port forward <pre><code> kubectl port-forward svc/svc_name -n namespace-name localhost-port:pod-port        \n kubectl port-forward svc/prometheus-operated -n zerone-monitoring 9090:9090                      \n</code></pre> Setup the Grafana</li> </ul> <ul> <li>Grafana Installation <pre><code> helm repo add grafana https://grafana.github.io/helm-charts                                      \n helm repo update                                                                                 \n helm install name grafana/grafana  --namespace namespace-name                                    \n</code></pre></li> <li>Get the password of the grafana <pre><code> kubectl  get secret grafana -n zerone-monitoring\n echo \u201cpassword_value\u201d  openssl base64 -d ; echo                                                 \n echo \u201cusername_value\u201d  openssl base64 -d ; echo \n kubectl port-forward svc/grafana -n zerone-monitoring 3000:3000 \n</code></pre></li> <li>Add the prometheus in the grafana dashboard. <pre><code>Prometheus server URL:  http://prometheus-operated:9090\nhttp://service-name:port\n</code></pre> </li> </ul>"},{"location":"Monitoring/1.%20setup_prometheus-grafana/#conclusion-after-setting-up-prometheus-and-grafana","title":"Conclusion After Setting Up Prometheus and Grafana","text":"<p>Setting up Prometheus and Grafana is a powerful combination for monitoring and visualizing your infrastructure and applications. After completing the setup, you can draw several conclusions and benefits:</p> <ol> <li> <p>Real-time Monitoring: With Prometheus, you have a powerful monitoring system that collects and stores metrics in real-time. This allows you to keep a close eye on the health and performance of your systems.</p> </li> <li> <p>Alerting: Prometheus provides alerting capabilities, allowing you to set up alerts based on predefined thresholds. You can be notified of issues before they become critical, helping you maintain system reliability.</p> </li> <li> <p>Historical Data: Prometheus stores historical metric data, enabling you to analyze trends and identify long-term performance patterns. This data can be invaluable for capacity planning and troubleshooting.</p> </li> <li> <p>Custom Metrics: You can instrument your applications and services to expose custom metrics, giving you deep insights into the specific aspects of your software that matter most to you.</p> </li> <li> <p>Grafana Dashboards: Grafana offers a user-friendly interface for creating and customizing dashboards that visualize Prometheus data. These dashboards can provide at-a-glance information about the state of your systems.</p> </li> <li> <p>Data Correlation: Grafana allows you to correlate data from multiple sources and display them in a single dashboard. This helps in diagnosing complex issues that involve multiple components.</p> </li> <li> <p>Flexibility: Both Prometheus and Grafana are highly customizable. You can adapt them to suit your unique monitoring needs, whether you're running a small web application or a large-scale distributed system.</p> </li> <li> <p>Community Support: Prometheus and Grafana have vibrant communities with active development and extensive documentation. You can find plugins, extensions, and support readily available online.</p> </li> <li> <p>Open Source: Both Prometheus and Grafana are open-source projects, meaning you can use them without incurring licensing costs. This makes them an economical choice for monitoring your infrastructure.</p> </li> <li> <p>Scalability: These tools are designed to scale horizontally, so you can expand your monitoring as your infrastructure grows.</p> </li> <li> <p>Integration: Prometheus and Grafana can integrate with various other tools and services, allowing you to centralize your monitoring and combine it with other aspects of your DevOps stack.</p> </li> </ol> <p>In conclusion, setting up Prometheus and Grafana provides you with a robust monitoring and visualization solution that empowers you to maintain the health and performance of your systems, make data-driven decisions, and respond proactively to issues. It's a valuable addition to any organization's infrastructure management toolkit. However, to maximize its benefits, ongoing maintenance and tuning may be required to ensure it continues to meet your evolving monitoring needs.</p>"},{"location":"Monitoring/2.%20gmail_integration/","title":"Gmail Integration","text":"<p>Description</p> <p>The application provides a Webhook integration for Prometheus AlertManager to push alerts to Google Chat rooms.</p> <ul> <li>Prometheus Installation <pre><code> helm repo add prometheus-community https://prometheus-community.github.io/helm-charts            \n helm repo update                                                                                 \n helm install name prometheus-community/prometheus --namespace namepsace-name    \n</code></pre></li> <li>Alertmanager-gchat-integration</li> </ul> <pre><code>A web application which listens for Prometheus AlertManager alerts' and forward them to Google \nChat rooms.\n</code></pre> <p><code>Install gchat manager using helm chart       helm repo add julb https://charts.julb.me   helm install name julb/alertmanager-gchat-integration  --namespace namespace-name</code></p> <ul> <li>Add the Gmail Webook in the prometheus-community-alertmanager</li> </ul> <pre><code>url : \u2018http://service.name(gchat-integration).namespace/alerts?room=lotus(roomname)\u2019\napiVersion: v1\ndata:\n  alertmanager.yml: |\n    global:\n      resolve_timeout: 1m\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'gmail'\n    receivers:\n    - name: 'gmail'\n      webhook_configs:\n      - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring/alerts?room=lotus'\n    templates:\n    - /etc/alertmanager/*.tmpl\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"data\":{\"alertmanager.yml\":\"global:\\n  resolve_timeout: 1m\\nroute:\\n  group_by: ['alertname']\\n  group_wait: 10s\\n  group_interval: 10s\\n  repeat_interval: 1h\\n  receiver: 'asd'\\nreceivers:\\n- name: 'asd'\\n  webhook_configs:\\n  - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring.svc/alerts?room=lotus'\\ntemplates:\\n- /etc/alertmanager/*.tmpl\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{\"meta.helm.sh/release-name\":\"prometheus-community\",\"meta.helm.sh/release-namespace\":\"zerone-monitoring\"},\"creationTimestamp\":\"2023-09-19T16:33:26Z\",\"labels\":{\"app.kubernetes.io/instance\":\"prometheus-community\",\"app.kubernetes.io/managed-by\":\"Helm\",\"app.kubernetes.io/name\":\"alertmanager\",\"app.kubernetes.io/version\":\"v0.26.0\",\"helm.sh/chart\":\"alertmanager-1.6.0\"},\"name\":\"prometheus-community-alertmanager\",\"namespace\":\"zerone-monitoring\",\"resourceVersion\":\"80702\",\"uid\":\"c171403d-1beb-44a0-a39b-b62a7472695f\"}}\n    meta.helm.sh/release-name: prometheus-community\n    meta.helm.sh/release-namespace: zerone-monitoring\n  creationTimestamp: \"2023-09-20T11:06:03Z\"\n  labels:\n    app.kubernetes.io/instance: prometheus-community\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/version: v0.26.0\n    helm.sh/chart: alertmanager-1.6.0\n  name: prometheus-community-alertmanager\n  namespace: zerone-monitoring\n  resourceVersion: \"95493\"\n  uid: c38ca1c3-0c69-48ea-8b3b-be724ab13c90\n</code></pre> <ul> <li>kubectl get cm prometheus\u2013community-server -o yaml</li> </ul> <pre><code>  alerting_rules.yml: |\n    groups:\n    - name: alertname\n      rules:\n      - alert: KubernetesNodeNotReady\n        expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n        for: 10m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes node not ready (instance {{ $labels.instance }})\n          description: \"Node {{ $labels.node }} has been unready for a long time\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesMemoryPressure\n        expr: kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes memory pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has MemoryPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesDiskPressure\n        expr: kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes disk pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has DiskPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesNetworkUnavailable\n        expr: kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes network unavailable (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has NetworkUnavailable condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesOutOfCapacity\n        expr: sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 &gt; 90\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes out of capacity (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} is out of capacity\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesContainerOomKiller\n        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes container oom killer (instance {{ $labels.instance }})\n          description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPersistentvolumeclaimPending\n        expr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n          description: \"PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPodCrashLooping\n        expr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})\n          description: \"Pod {{ $labels.pod }} is crash looping\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: ImagePullBackoffAlert\n        expr: kube_pod_container_status_waiting_reason{reason=\"ImagePullBackOff\"} == 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is in ImagePullBackOff state\"\n          description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing to pull its container image.\"\n</code></pre> <ul> <li>Get the google chat space url </li> </ul> <p></p> <p></p> <ul> <li> <p>Encode  the following to base64. <pre><code>[app.notification]\n# Jinja2 custom template to print message to GChat.\ncustom_template_path = \"/opt/alertmanager-gchat-integration/cm/notification-template-json.j2\"\n[app.room.lotus(google_space_name)]\nnotification_url = \u2018&lt;google_chat_space_url&gt;\u2019\n</code></pre></p> </li> <li> <p>Use the convert base64 code in the Secret (gchat-integration) &amp;&amp; paste  in config.toml section <pre><code>apiVersion: v1\ndata:\n  config.toml: &lt;Encoded base64 value&gt;&gt;\nkind: Secret\n</code></pre></p> </li> <li> <p>After updating the Secret and ConfigMap we need to restart some Pod:  <pre><code>    - alert-manager-alertmanager-gchat-integration\n    - prometheus-community-alertmanager\n    - prometheus-community-server\n</code></pre></p> </li> </ul> <p>Output</p> <p></p>"},{"location":"Monitoring/3.%20slack_integration/","title":"Slack Integration","text":"<p>Setup the Prometheus and Grafana</p> <p>Generating the Webhook of the Slack.</p> <p>Go to app and create a new slack notifications webhooks.</p> <p> </p> <ul> <li> <p>kubectl get cm prometheus-community-alertmanager -o yaml <pre><code> alertmanager.yml: |\n    global:\n      resolve_timeout: 1m\n      slack_api_url: 'https://hooks.slack.com/services/T023XD85BFA/B05XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG00vrsFjos50iid'\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'slack-notifications'\n    receivers:\n    - name: 'slack-notifications'\n      slack_configs:\n      - channel: '#&lt;add here the name of the slack channel&gt;'\n        send_resolved: true\n        icon_url: https://avatars3.githubusercontent.com/u/3380462\n        icon_emoji: ':fire:'\n        title: |-\n          [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}\n          {{- if gt (len .CommonLabels) (len .GroupLabels) -}}\n            {{\" \"}}(\n            {{- with .CommonLabels.Remove .GroupLabels.Names }}\n              {{- range $index, $label := .SortedPairs -}}\n                {{ if $index }}, {{ end }}\n                {{- $label.Name }}=\"{{ $label.Value -}}\"\n              {{- end }}\n            {{- end -}}\n            )\n          {{- end }}\n        text: |-\n          {{ range .Alerts -}} *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}\n          *Description:* {{ .Annotations.description }}\n          *Details:*\n            {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}`\n            {{ end }}\n          {{ end }}\n    templates:\n    - /etc/alertmanager/*.tmpl\n</code></pre></p> </li> <li> <p>kubectl get cm prometheus\u2013community-server -o yaml</p> </li> </ul> <pre><code>  alerting_rules.yml: |\n    groups:\n    - name: alertname\n      rules:\n      - alert: KubernetesNodeNotReady\n        expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n        for: 10m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes node not ready (instance {{ $labels.instance }})\n          description: \"Node {{ $labels.node }} has been unready for a long time\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesMemoryPressure\n        expr: kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes memory pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has MemoryPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesDiskPressure\n        expr: kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes disk pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has DiskPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesNetworkUnavailable\n        expr: kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes network unavailable (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has NetworkUnavailable condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesOutOfCapacity\n        expr: sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 &gt; 90\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes out of capacity (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} is out of capacity\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesContainerOomKiller\n        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes container oom killer (instance {{ $labels.instance }})\n          description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPersistentvolumeclaimPending\n        expr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n          description: \"PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPodCrashLooping\n        expr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})\n          description: \"Pod {{ $labels.pod }} is crash looping\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: ImagePullBackoffAlert\n        expr: kube_pod_container_status_waiting_reason{reason=\"ImagePullBackOff\"} == 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is in ImagePullBackOff state\"\n          description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing to pull its container image.\"\n</code></pre> <ul> <li>After Completing the above steps it shows alerts like below example</li> </ul> <p></p> <ul> <li>For both Gmail and Slack <pre><code>apiVersion: v1\ndata:\n  alertmanager.yml: |-\n    global:\n      resolve_timeout: 1m\n      slack_api_url: 'https://hooks.slack.com/services/T023XD85BFA/B05XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG00vrsFjos50iid'\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'slack-notifications'\n      routes:\n        # Route for Gmail notifications\n        - receiver: 'gmail-notification'\n          match:\n            severity: 'critical'  # Define the condition to route to Gmail\n          continue: true  # Continue processing other routes\n        # Route for Slack notifications\n        - receiver: 'slack-notifications'\n          match:\n             severity: 'critical'\n          continue: true\n    receivers:\n    - name: 'slack-notifications'\n      slack_configs:\n      - channel: '#lotus'\n        send_resolved: true\n        icon_url: https://avatars3.githubusercontent.com/u/3380462\n        icon_emoji: ':fire:'\n        title: |-\n          [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}\n          {{- if gt (len .CommonLabels) (len .GroupLabels) -}}\n            {{ \" \" }}(\n            {{- with .CommonLabels.Remove .GroupLabels.Names }}\n              {{- range $index, $label := .SortedPairs -}}\n                {{ if $index }}, {{ end }}\n                {{- $label.Name }}=\"{{ $label.Value -}}\"\n              {{- end }}\n            {{- end -}}\n            )\n          {{- end }}\n        text: |-\n          {{ range .Alerts -}} *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}\n          *Description:* {{ .Annotations.description }}\n          *Details:*\n            {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}`\n            {{ end }}\n          {{ end }}\n    - name: 'gmail-notification'\n      webhook_configs:\n      - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring.svc/alerts?room=lotus'\n    templates:\n    - /etc/alertmanager/*.tmpl\nkind: ConfigMap\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: prometheus-community\n    meta.helm.sh/release-namespace: zerone-monitoring\n  creationTimestamp: \"2023-09-19T16:33:26Z\"\n  labels:\n    app.kubernetes.io/instance: prometheus-community\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/version: v0.26.0\n    helm.sh/chart: alertmanager-1.6.0\n  name: prometheus-community-alertmanager\n  namespace: zerone-monitoring\n  resourceVersion: \"80702\"\n  uid: c171403d-1beb-44a0-a39b-b62a7472695f\n</code></pre></li> </ul> <p>References <pre><code>https://artifacthub.io/packages/helm/prometheus-community/prometheus\nhttps://medium.com/globant/setup-prometheus-and-grafana-monitoring-on-kubernetes-cluster-using-helm-3484efd85891\nhttps://grafana.com/blog/2020/02/25/step-by-step-guide-to-setting-up-prometheus-alertmanager-with-slack-pagerduty-and-gmail/\nhttps://samber.github.io/awesome-prometheus-alerts/rules#kubernetes\n</code></pre></p>"},{"location":"Security/KubeScape/","title":"KubeScape","text":"<p>Kubescape is an open source Kubernetes security platform for your IDE, CI/CD pipelines, and clusters. Kubescape includes risk analysis, security compliance, and misconfiguration scanning.</p>"},{"location":"Security/KubeScape/#installing-kubescape","title":"Installing Kubescape","text":"<p><pre><code>curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash \n</code></pre> </p> <p>Run a scan <pre><code>kubescape scan --enable-host-scan --verbose\n\n\u2018kubescape scan\u2019 : command to scan k8s clusters\n\u2018--enable-host-scan\u2019:  scan not just the Kubernetes cluster itself, but also the underlying hosts on which the cluster is running.\n\u2018--verbose\u2019: to provide additional information during the scan.\n</code></pre></p> <p></p> <pre><code>        Figure: Scanning the k8s cluster for Security risk and Vulnerabilities\n</code></pre> <ul> <li> <p>Get results!</p> </li> </ul> <p></p> <pre><code>        Figure: List of Security risk and Vulnerabilities\n</code></pre>"},{"location":"Security/KubeScape/#armosec-with-kubescape","title":"ArmoSec with KubeScape","text":"<ul> <li> <p>Naviagte to cloud.armosec.io/account</p> </li> </ul> <p></p> <ul> <li> <p>Connect the K8s Cluster</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>Check the compliance</p> </li> </ul> <p></p> <ul> <li> <p>Check Dashboard for k8s cluster</p> </li> </ul> <p></p>"},{"location":"Security/KubeScape/#architecture-of-kubescape","title":"Architecture of Kubescape","text":"<p>The main components of the Kubescape architecture are:</p> <ul> <li> <p>Scanner:</p> <p>This is the core component of Kubescape. It scans the Kubernetes cluster and identifies potential security risks and vulnerabilities. It uses different techniques to analyze the Kubernetes configuration and resources, such as identifying misconfigured network policies, insecure container images, and non-compliant Kubernetes objects.</p> </li> <li> <p>Policies: </p> <p>Kubescape uses a set of predefined policies to evaluate the Kubernetes environment. These policies are customizable and can be modified to suit the specific security requirements of an organization. The policies are written in YAML and can be easily extended to include additional checks.</p> </li> <li> <p>Result: </p> <p>The results component of Kubescape provides a detailed report of the security risks and vulnerabilities identified during the scan. It categorizes the findings based on their severity and provides remediation steps for each issue.</p> </li> <li> <p>Web interface: </p> <p>Kubescape comes with a web interface that allows users to interact with the tool and view the scan results. The interface provides an easy-to-use dashboard that summarizes the scan findings and allows users to drill down into specific issues.</p> </li> <li> <p>API:</p> <p>Kubescape also exposes an API that can be used to integrate with other security tools and automate the scanning process. The API can be used to trigger scans, retrieve scan results, and perform other actions programmatically.</p> </li> </ul>"},{"location":"Security/KubeScape/#usage-of-kubescape","title":"Usage of Kubescape","text":"<ul> <li> <p>Security auditing: </p> <p>Kubescape can be used to audit the security of a Kubernetes environment by scanning for security risks and vulnerabilities. It can help security teams to identify potential security issues and take the necessary steps to remediate them.</p> </li> <li> <p>Compliance testing: </p> <p>Kubescape can also be used to test Kubernetes clusters against compliance standards such as PCI-DSS, HIPAA, or SOC 2. It can help organizations to ensure that their Kubernetes environments are compliant with industry standards and regulations.</p> </li> <li> <p>DevOps integration: </p> <p>Kubescape can be integrated into the DevOps pipeline to perform security checks automatically. This can help organizations to ensure that their Kubernetes deployments are secure and compliant before they are deployed.</p> </li> <li> <p>Continuous Monitoring: </p> <p>Kubescape can be used to continuously monitor the security of a Kubernetes environment by running scheduled scans. This can help organizations to detect and remediate security risks and vulnerabilities in a timely manner.</p> </li> </ul>"},{"location":"Security/KubeScape/#demonstration","title":"Demonstration","text":"<p>Installation:</p> <p><pre><code>curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash \n</code></pre> </p> <pre><code>kubescape scan  --enable-host-scan\n</code></pre> <p> </p> <pre><code>    Scan the whole cluster in the system\n</code></pre> <ul> <li> <p>Create and Deploy any Kubernetes Deployment file in the cluster</p> </li> </ul> <p></p> <ul> <li> <p>Scan the created deployment file <pre><code>kubescape scan filename\n</code></pre></p> </li> </ul> <p></p> <pre><code>    Scanning the deployment file with vulnerabilities\n</code></pre> <ul> <li> <p>Solving the Resources Limit vulnerabilities</p> </li> </ul> <p></p> <pre><code>    Adding the limit and request resources inside the deployment yaml file\n</code></pre> <ul> <li> <p>Resources Limit Vulnerabilities problem solved</p> </li> </ul> <p></p> <p>Figure: Resources Limit Vulnerabilities Solved</p> <ul> <li> <p>Uploading the severity in the json format <pre><code>kubescape scan nginx-deploy.yaml --format json --output op.json\n</code></pre> </p> <pre><code>Saved the op.json file\n</code></pre> </li> </ul>"},{"location":"Security/M9Sweeper/","title":"M9Sweeper","text":""},{"location":"Security/M9Sweeper/#introduction","title":"Introduction","text":"<p>M9sweeper is a free and easy kubernetes security platform.    It integrates industry standard open source utilities into a one-stop-shop kubernetes security tool that can walk most kubernetes administrators through securing a kubernetes cluster as well as the apps running on the cluster.</p>"},{"location":"Security/M9Sweeper/#feature","title":"Feature","text":"<p>M9sweeper makes securing a cluster easy with:</p> <ul> <li>CVE Scanning</li> <li>Enforcement of CVE Scanning Rules</li> <li>Reports and Dashboards</li> <li>CIS Security Benchmarking</li> <li>Pen Testing</li> <li>Deployment Coaching</li> <li>Intrusion Detection</li> <li>Gatekeeper Policy Management</li> </ul>"},{"location":"Security/M9Sweeper/#prerequisites","title":"\u203c\ufe0f Prerequisites","text":"<ul> <li>This project requires a Kubernetes Cluster and uses helm as package manager  Mac Install from terminal</li> </ul>"},{"location":"Security/M9Sweeper/#for-intel-macs","title":"For Intel Macs","text":"<pre><code>[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-amd64\n</code></pre>"},{"location":"Security/M9Sweeper/#for-m1-arm-macs","title":"For M1 / ARM Macs","text":"<pre><code>[ $(uname -m) = arm64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-arm64\nchmod +x ./kind\nmv ./kind /some-dir-in-your-PATH/kind\n</code></pre>"},{"location":"Security/M9Sweeper/#windows","title":"Windows","text":"<pre><code>Install with chocolatey and install a bash client\n\nchoco install kind\nchoco install git\n</code></pre>"},{"location":"Security/M9Sweeper/#start-kubernetes","title":"Start Kubernetes","text":"<pre><code>kind create cluster --name cluster_name\n</code></pre> <p>Quick Installation for the M9Sweeper</p> <pre><code>helm repo add m9sweeper https://m9sweeper.github.io/m9sweeper \nhelm repo update \n helm install  m9sweeper m9sweeper/m9sweeper --install --wait \\\n  --create-namespace --namespace m9sweeper-system \\\n  --set-string dash.init.superAdminEmail=\"email\" \\\n  --set-string dash.init.superAdminPassword=\"password\" \\\n  --set-string global.jwtSecret=\"changeme\" \\\n  --set-string global.apiKey=\"ffb40975b2694226\"\n</code></pre>"},{"location":"Security/M9Sweeper/#cve-scanning","title":"CVE Scanning","text":"<ul> <li>Common Vulnerability Exposures/Enumeration gives common names to openly known security issues or vulnerabilities. </li> <li>The objective of CVE is to make it simpler to impart information over different databases and make available a common platform to evaluate security tools.</li> </ul>"},{"location":"Security/M9Sweeper/#trivy","title":"Trivy","text":"<ul> <li>Trivy is a comprehensive and versatile security scanner. </li> <li>Trivy has scanners that look for security issues, and targets where it can find those issues.</li> </ul> <p>Targets (what Trivy can scan):</p> <ul> <li>Container Image</li> <li>Filesystem</li> <li>Git Repository (remote)</li> <li>Virtual Machine Image</li> <li>Kubernetes</li> <li>AWS</li> </ul> <p>Scanners (what Trivy can find there):</p> <ul> <li>OS packages and software dependencies in use (SBOM)</li> <li>Known vulnerabilities (CVEs)</li> <li>IaC issues and misconfigurations</li> <li>Sensitive information and secrets</li> <li>Software licences</li> </ul> <p>How CVE Scanning works in M9Sweeper</p> <ul> <li>Login into the M9Sweeper</li> <li>After Logged in the below dashboard is visible  Fig: M9Sweeper Dashboard</li> <li>Choose the cluster for security checkup   Fig: Default cluster already present in the dashboard</li> <li>After choosing the cluster navigate into the Images  in the dashboard  Fig: List of Images present in the dashboard  Fig: We can manually add and scan the images using the URL  Fig: Output of the manual scan image </li> </ul> <p>Here Can check the severity level and CVE Types explaining about the CVE Vulnerabilities Weakness,Affected Software,Potential Mitigations and References with score level. See the details of the Compliant report  Apply for Request Exception</p> <ul> <li> <p>Enforcement of CVE Scanning Rules     Enforcement of Common Vulnerabilities and Exposures (CVE) scanning rules is a critical aspect of maintaining the security of computer systems and networks.   Reports and Dashboards, including historical reporting to see how your security posture has changed over time  Fig: Report summary on the dashboard</p> </li> <li> <p>CIS Security Benchmarking (Center for Internet Security)</p> </li> <li>Kube-bench    kube-bench is a tool that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark.</li> </ul> <p>In the context of kube-bench, \"Standard Kubernetes Environment - Just Master Benchmarks\" refers to a specific configuration or set of security benchmarks that are focused on the control plane components (master components) of a Kubernetes cluster. </p> <p> Fig: Selecting the k8s env for benchmark  Fig: Installation of kube-bench   Fig: Installation Output for the benchmark   Fig: Output</p> <p>Pen Testing       Assess whether you are secure by running a regular, scheduled non-invasive penetration test using kube-hunter. Results can be reviewed in the user interface.   Kube-hunter   kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.</p> <p> Fig: Installation command   Fig: Installation of kube-hunter   Fig: Waiting for the result   Fig: Result Output </p> <p>Deployment Coachings   Ensure deployments are properly secured by reviewing your application deployments with kubesec. Ratings and summary reports are available in a simple graphical user interface.  Fig: Choose the pods from cluster or file   Fig: Reports generate after scanning the cluster</p> <p>Intrusion Detection   Monitor applications for suspicious activity and/or intrusions by hackers (such as reverse shells) using Project Falco. Detect anomalies by receiving alerts whenever a new kind of suspicious behaviour is detected.</p> <p>Project Falco     Falco is a cloud native runtime security tool for Linux operating systems. It is designed to detect and alert on abnormal behavior and potential security threats in real-time.</p> <p>At its core, Falco is a kernel monitoring and detection agent that observes events, such as syscalls, based on custom rules. Falco can enhance these events by integrating metadata from the container runtime and Kubernetes. The collected events can be analyzed off-host in SIEM or data lake systems.  Fig: Installation of Falco</p>"},{"location":"Security/M9Sweeper/#gatekeeper-policy-management","title":"Gatekeeper Policy Management","text":"<p>Gatekeeper is an admission controller that validates requests to create and update Pods on Kubernetes clusters, using the Open Policy Agent (OPA).    Using Gatekeeper allows administrators to define policies with a constraint, which is a set of conditions that permit or deny deployment behaviors in Kubernetes.   Policy Controller for Kubernetes.</p> <ul> <li>Installation   Fig: Install the Gatekeeper  Note (Gatekeeper for workloads else internal server error)  Fig: copy the command given in the clipboard and paste in the terminal   Fig:GateKeeper Installed in the Cluster </li> </ul>"},{"location":"Traefik/Traefik/","title":"Traefik","text":""},{"location":"Traefik/Traefik/#introduction","title":"Introduction","text":"<p>Traefik is based on the concept of EntryPoints, Routers, Middlewares and Services.</p> <p>The main features include dynamic configuration, automatic service discovery, and support for multiple backends and protocols.</p> <p>EntryPoints: EntryPoints are the network entry points into Traefik. They define the port which will receive the packets, and whether to listen for TCP or UDP.</p> <p>Routers: A router is in charge of connecting incoming requests to the services that can handle them.</p> <p>Middlewares: Attached to the routers, middlewares can modify the requests or responses before they are sent to your service</p> <p>Services: Services are responsible for configuring how to reach the actual services that will eventually handle the incoming requests.</p> <p>Applied some simple postgres manifest for the TCP routing </p> <p></p> <p>Dashboard Image with the entrypoints </p> <p></p> <p>Exposing the Entrypoints using the postgres </p> <p></p>"},{"location":"Traefik/Traefik/#installation","title":"Installation","text":"<pre><code>helm repo add traefik https://helm.traefik.io/traefik\nhelm repo update\nhelm install traefik traefik/traefik --version 28.3.0 -f values.yaml\n\nadditionalArguments: \n  - \"--entryPoints.postgres.address=:5432\"\n  - \"--entrypoints.redis.address=:6379\"\n  - \"--entrypoints.mariadb.address=:3306\"\n  - \"--certificatesresolvers.le-staging.acme.tlschallenge=true\"\n  - \"--certificatesresolvers.le-staging.acme.email=xxx@gmail.com\"\n  - \"--certificatesresolvers.le-staging.acme.storage=/data/acme.json\"\n  - \"--certificatesresolvers.le-staging.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directory\"\n #  - \"--providers.kubernetesingress.ingressclass=traefik-internal\"\n  - \"--log.level=DEBUG\"\nports:\n  redis:\n    port: 6379\n    protocol: TCP\n    expose: \n      default: true\n    tls:\n      enabled: true    \n  postgres:\n    expose:\n      default: true\n    exposePort: 5432\n    port: 5432\n    protocol: TCP\n    transport:\n      respondingTimeouts:\n        readTimeout: 0\n        writeTimeout: 0\n        idleTimeout: 0\n      keepAliveMaxRequests: 0\n      keepAliveMaxTime: 0\n  mariadb:\n    expose:\n      default: true\n    exposePort: 3306\n    port: 3306\n    protocol: TCP\n    transport:\n      respondingTimeouts:\n        readTimeout: 0\n        writeTimeout: 0\n        idleTimeout: 0\n      keepAliveMaxRequests: 0\n      keepAliveMaxTime: 0\n</code></pre>"},{"location":"Traefik/Traefik/#redis","title":"Redis","text":"<pre><code>Installation\n\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-redis bitnami/redis --version 19.5.5\n</code></pre> <ul> <li> <p>Manifest the IngressRouteTCP for Redis  <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: redis-service-tcp-clone-1\n  namespace: redis\nspec:\n  entryPoints:\n    - redis\n  routes:\n  - match: HostSNI(`redis1.db.rajivgopalsingh.com.np`)\n    services:\n    - name: redis-master\n      namespace: redis\n      port: 6379\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre></p> </li> <li> <p>Test the Redis server using the CLI  <pre><code>redis-cli -h redis1.db.rajivgopalsingh.com.np -p 6379 -a qk50H1nOdY --tls --sni redis1.db.rajivgopalsingh.com.np\n</code></pre> Note:  <pre><code>when changing the values of the tls.passthrough: true occurs\n Error: Protocol error, got \"H\" as reply type byte\n</code></pre></p> </li> </ul>"},{"location":"Traefik/Traefik/#postgresql","title":"PostgreSQL","text":"<p>Tested with the Crunchy Postgres and CloudNative PG  CrunchyData PG</p> <pre><code>Installation:\nkubectl create -f https://operatorhub.io/install/postgresql.yaml\n</code></pre> <p>CloudNative PG </p> <pre><code>Installation:\nkubectl create -f https://operatorhub.io/install/cloudnative-pg.yaml\n</code></pre> <p>Bitnami PG     helm repo add bitnami https://charts.bitnami.com/bitnami     helm install my-postgresql bitnami/postgresql --version 15.5.7</p> <p>IngressRouteTCP manifest for the PG <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: deployment-625-postgres-ha\n  namespace: env-203\nspec:\n  entryPoints:\n    - postgres\n  routes:\n  - match: HostSNI(`*`)\n    services:\n    - name: deployment-625-postgres-ha\n      namespace: env-203\n      port: 5432\n  tls:\n    certResolver: le-staging\n    domains:                   \n    - main: db.rajivgopalsingh.com.np        \n      sans:                    \n      - postgres625.db.rajivgopalsingh.com.np\n      - post625.db.rajivgopalsingh.com.np\n    passthrough: true \n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: deployment-626-postgres-ha\n  namespace: env-203\nspec:\n  entryPoints:\n    - postgres\n  routes:\n  - match: HostSNI(`postgres626.db.rajivgopalsingh.com.np`)\n    services:\n    - name: deployment-626-postgres-ha\n      namespace: env-203\n      port: 5432\n  tls:\n    certResolver: le-staging\n    passthrough: true \n</code></pre> Output</p> <p></p>"},{"location":"Traefik/Traefik/#mariadb","title":"MariaDB","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-mariadb bitnami/mariadb --version 18.2.4\n</code></pre> <p>Manifest IngressRouteTCP for the MariaDB</p> <pre><code> apiVersion: traefik.io/v1alpha1\n kind: IngressRouteTCP\n metadata:\n   name: deployment-628-mariadb\n   namespace: env-203\n spec:\n   entryPoints:\n     - mariadb\n   routes:\n   - match: HostSNI(`*`)\n     services:\n     - name: deployment-628-mariadb\n       namespace: env-203\n       port: 3306\n</code></pre> <pre><code>Currently while setuping the MariaDB the above manifest is working only if we are providing the  wildcard in the HOSTSNI\n</code></pre> <p>Output</p> <p></p> <p>Mongodb     helm repo add bitnami https://charts.bitnami.com/bitnami     helm install my-mongodb bitnami/mongodb --version 15.6.11</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: mongodb\nspec:\n  entryPoints:\n    - mongodb\n  routes:\n    - match: HostSNI(`mongodb.db.umeshkhatiwada.com.np`)\n    priority: 10\n    services:\n        - name: my-mongodb\n        port: 27017\n        weight: 10\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre> <p>Cmd: mongo --host host1.com --port 27017 -u root -p cRIt05TMRr --authenticationDatabase admin --ssl --sslAllowInvalidCertificates</p>"},{"location":"Traefik/Traefik/#rabbitmq","title":"Rabbitmq","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-rabbitmq bitnami/rabbitmq --version 14.4.4\n</code></pre> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: rabbitmq\nspec:\n  entryPoints:\n    - amqp\n  routes:\n    - match: HostSNI(`rabbitmq.db.umeshkhatiwada.com.np`)\n    priority: 10\n    services:\n        - name: my-rabbitmq\n        port: 5672\n        weight: 10\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre> <p>Testscript: <pre><code>import pika\nimport ssl\nimport socket\n\n# RabbitMQ server configuration\nhostname = 'host1.com'  # Replace with your RabbitMQ server hostname or IP address\nport = 5672  # Default RabbitMQ SSL port\nusername = 'user'  # RabbitMQ username\npassword = 'BE9zlp2MQugpQVbY'  # RabbitMQ password\nvirtual_host = '/'  # RabbitMQ virtual host\n\n# SSL/TLS configuration\ncontext = ssl.create_default_context()\n\n# Disable hostname checking and certificate verification for self-signed certificates\ncontext.check_hostname = False\ncontext.verify_mode = ssl.CERT_NONE\n\n# TCP Keep-Alive settings\nsocket.setdefaulttimeout(10)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 5)\n\n# Connect to RabbitMQ server with SSL\ncredentials = pika.PlainCredentials(username, password)\nparameters = pika.ConnectionParameters(\n    host=hostname,\n    port=port,\n    virtual_host=virtual_host,\n    credentials=credentials,\n    ssl_options=pika.SSLOptions(context)\n)\ntry:\n    connection = pika.BlockingConnection(parameters)\n    channel = connection.channel()\n\n    # Declare a queue\n    queue_name = 'hello'\n    channel.queue_declare(queue=queue_name)\n\n    # Publish a message to the queue\n    message = 'Hello, RabbitMQ with SSL!'\n    channel.basic_publish(exchange='', routing_key=queue_name, body=message)\n    print(f\" [x] Sent '{message}'\")\n\n    # Close connection\n    connection.close()\nexcept Exception as e:\n    print(f\"Failed to connect to RabbitMQ server: {e}\")\n</code></pre></p>"},{"location":"Traefik/Traefik/#reference","title":"Reference","text":"<ul> <li> <p>Is it possible to use Traefik to proxy PostgreSQL over SSL?</p> </li> <li> <p>\ud83d\udd28 Support Traefik v3 features \u00b7 Issue #757 \u00b7 traefik/traefik-helm-chart </p> </li> <li> <p>How to connect to Traefik TCP Services with TLS configuration enabled? </p> </li> <li> <p>Release v3.0.0-rc5 \u00b7 traefik/traefik </p> </li> <li> <p>Using Traefik as Ingress on different MetalLB loadbalancerIP? </p> </li> <li> <p>Exposing a postgres service through Traefik IngressRouteTCP and Cloudflare DNS </p> </li> </ul>"},{"location":"Traefik/Traefik/#workshop","title":"Workshop","text":"<ul> <li> <p>GitHub - traefik-workshops/traefik-workshop</p> <p>Getting started with Traefik on Kubernetes. A few exercises that help you to start the journey with Traefik Proxy. </p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/","title":"How to rotate and renew expired Contour secrets (contourcert and envoycert)","text":"<p>The purpose  is to provide a detailed guide on how to rotate and renew expired secrets (contourcert and envoycert) under projectcountour namespace. It includes a step-by-step workaround, prerequisites, and verification steps.</p>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#symptoms","title":"Symptoms:","text":"<ul> <li> <p>Envoy (pods with name starting with \"projectcontour-envoy-\") pods under projectcontour namespace show restarts.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#example","title":"Example","text":"<pre><code>kubectl get pods -n proejctcontour\n\nNAMESPACE        NAME                                     READY  STATUS    RESTARTS  AGE  \nprojectcontour   projectcontour-contour-c476b465b-b5bvn    1/1   Running    1        16h  \nprojectcontour   projectcontour-contour-c476b465b-jkhsd    1/1   Running    1        16h                    \nprojectcontour   projectcontour-envoy-wegsv                1/2   Running    161      16h          \nprojectcontour   projectcontour-envoy-sd7xc                1/2   Running    161      16h      \nprojectcontour   projectcontour-envoy-u9hac                1/2   Running    162      16h\n</code></pre> <ul> <li> <p>The secrets (contourcert and envoycert) in projectcontour namespace are expired.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#example_1","title":"Example","text":"<pre><code>kubectl  get secret -n projectcontour envoycert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\nnotBefore=Jul 24 19:11:31 2023 GMT\nnotAfter=Jul 26 19:11:31 2024 GMT\n\nkubectl  get secret -n projectcontour contourcert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\nnotBefore=Jul 24 19:11:31 2023 GMT\nnotAfter=Jul 26 19:11:31 2024 GMT\n</code></pre> <ul> <li> <p>The log for envoy pods in the projectcontour namespace (pods with name starting with \"projectcontour-envoy-\") reports TLS errors, such as: <pre><code>  kubectl  logs -n projectcontour &lt;envoy pod name&gt;\n</code></pre></p> </li> <li> <p>[2024-07-26 16:32:01.575][1][warning][config] [bazel-out/k8-opt/bin/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:101] StreamListeners gRPC config stream closed: 14, upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#cause","title":"Cause","text":"<ul> <li> <p>The secrets (contourcert and envoycert) are used for internal gRPC communication between Contour and Envoy.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#update-the-contour-and-under-all-the-resources-defined-in-contour_certgen_jobyaml-which-has-following-tags","title":"Update the contour  and  under all the resources defined in \"contour_certgen_job.yaml\" which has following tags: <pre><code>\"helm.sh/chart: contour-&lt;version&gt;\"\n\"image: docker.io/bitnami/contour:&lt;image_version&gt;\"\nYou can get the current version by running \"kubectl describe\" on contour deployment/pods.\n</code></pre> <p>Run the following commands from manager node to rotate/renew the expired contourcert/envoycert (you can run these commands on the control plane node, there is no need to pass kubeconfig option in that case):</p> <pre><code>1. Take backup:\n    kubectl get secret -n projectcontour contourcert -o yaml &gt; contourcert-backup.yaml\n  kubectl get secret -n projectcontour envoycert -o yaml &gt; envoycert-backup.yaml\n\n2. Delete current contourcert and envoycert:\n   kubectl delete secret -n projectcontour envoycert contourcert\n\n3. Generate new contourcert and envoycert :\n    kubectl apply -f &lt;path&gt;/contour_certgen_job.yaml -n projectcontour\n\n4. Verify that the new contourcert/envoycert was generated:\n    kubectl get secret -n projectcontour\n\n5. Verify that the validity period of new secrets is 10 years:\n  kubectl get secret -n projectcontour envoycert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\n  kubectl get secret -n projectcontour contourcert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\n\n6. Restart all contour pods:\n   kubectl patch deployment projectcontour-contour -n projectcontour -p '{\"spec\": {\"template\": {\"metadata\": {\"labels\":{\"test\": \"restart\"} } } } }' --type=merge\n\n7. Restart all envoy pods:\n  kubectl patch daemonset projectcontour-envoy -n projectcontour -p '{\"spec\": {\"template\": {\"metadata\": {\"labels\":{\"test\": \"restart\"} } } } }' --type=merge\n\n8. Delete projectcontour-contour-certgen job if it is present:\n  kubectl get job projectcontour-contour-certgen -n projectcontour\n\n    If the output of above command shows the projectcontour-contour-certgen job, then run below command to delete it: \n  kubectl delete job projectcontour-contour-certgen -n projectcontour\n\n9. Verify all contour/envoy pods are in running state and not restarting:\n  kubectl get pods -n projectcontour\n</code></pre>","text":""},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#manifest","title":"Manifest <pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\nspec:\n  allowPrivilegeEscalation: true\n  allowedCapabilities:\n  - '*'\n  fsGroup:\n    rule: RunAsAny\n  hostIPC: true\n  hostNetwork: true\n  hostPID: true\n  hostPorts:\n  - max: 65535\n    min: 0\n  privileged: true\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n  - '*'\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - projectcontour-contour-certgen-psp\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: projectcontour-contour-certgen-psp\nsubjects:\n  - kind: ServiceAccount\n    name: projectcontour-contour-certgen\n    namespace: zerone-projectcontour\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: projectcontour-contour-certgen\nsubjects:\n  - kind: ServiceAccount\n    name: projectcontour-contour-certgen\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: contour\n        helm.sh/chart: contour-10.2.2\n        app.kubernetes.io/instance: projectcontour\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: contour-certgen\n    spec:\n\n      containers:\n        - name: contour\n          image: docker.io/bitnami/contour:1.23.3-debian-11-r0\n          imagePullPolicy: IfNotPresent\n          command:\n            - contour\n          args:\n            - certgen\n            - --kube\n            - --incluster\n            - --overwrite\n            - --secrets-format=compact\n            - --namespace=$(CONTOUR_NAMESPACE)\n            - --certificate-lifetime=3650\n          env:\n            - name: CONTOUR_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          resources:\n            limits:\n              memory: 256Mi\n            requests:\n              cpu: 40m\n              memory: 32Mi\n      restartPolicy: Never\n      serviceAccountName: projectcontour-contour-certgen\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        fsGroup:\n        runAsNonRoot: true\n  parallelism: 1\n  completions: 1\n  backoffLimit: 1\n</code></pre>","text":""}]}