{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A one-stop hub for all your DevOps documentation needs, containing helpful guides, how-to tutorials, and top-notch best practices.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p>Monitoring</p> </li> <li> <p>Logging </p> </li> <li> <p>Backup &amp; Restore</p> </li> <li> <p>Security</p> </li> <li> <p>Multi-Cluster</p> </li> <li> <p>Traefik</p> </li> </ul>"},{"location":"Backup%26Restore/kasten/","title":"Kasten K10 Installation on Kind","text":""},{"location":"Backup%26Restore/kasten/#prerequisites","title":"Prerequisites","text":"<ul> <li>kind v0.8.1 or higher</li> <li>Kubernetes v1.17 or higher</li> <li>VolumeSnapshot beta CRDs and the Snapshot Controller</li> <li>The CSI Hostpath Driver<pre><code>https://github.com/kubernetes-csi/csi-driver-host-path\n</code></pre> </li> </ul>"},{"location":"Backup%26Restore/kasten/#installation-setup","title":"Installation Setup","text":"<p>Before proceeding, ensure you have the following prerequisites in place: -   Kind: v0.8.1 or higher -   Kubernetes: v1.17 or higher -   VolumeSnapshot CRDs and the Snapshot Controller:  These components are essential for managing volume snapshots in Kubernetes. -   CSI Hostpath Driver: A Container Storage Interface (CSI) driver that provisions storage directly from the host's filesystem. This is suitable for Kind clusters.</p>"},{"location":"Backup%26Restore/kasten/#1-install-kubernetes-with-kind","title":"1. Install Kubernetes with Kind","text":"<ul> <li>Create a Kubernetes cluster using Kind. We'll use Kubernetes v1.25.8 in this example:</li> </ul>"},{"location":"Backup%26Restore/kasten/#apply-volumesnapshot-crds","title":"Apply VolumeSnapShot CRDS","text":"<p>Ref Link</p> <pre><code> https://github.com/kubernetes-csi/external-snapshotter/tree/master/client/config/crd\n</code></pre> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml\n</code></pre>"},{"location":"Backup%26Restore/kasten/#create-snapshot-controller","title":"Create Snapshot Controller","text":"<p>Ref Link</p> <pre><code>https://github.com/kubernetes-csi/external-snapshotter/tree/master/deploy/kubernetes/snapshot-controller\n</code></pre> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml\n</code></pre>"},{"location":"Backup%26Restore/kasten/#install-the-csi-hostpath-driver","title":"Install the CSI Hostpath Driver","text":"<p><pre><code>git clone https://github.com/kubernetes-csi/csi-driver-host-path.git\n\ncd csi-driver-host-path\n\n./deploy/kubernetes-1.25/deploy.sh\n</code></pre> After the install is complete, add the CSI Hostpath Driver StorageClass and make it the default</p> <p>Apply the yaml file from the external-snapshotter/examples/kubernetes</p>"},{"location":"Backup%26Restore/kasten/#configure-csi-hostpath-driver-storageclass","title":"Configure CSI Hostpath Driver StorageClass","text":"<pre><code>kubectl apply -f ./examples/csi-storageclass.yaml\n\nkubectl patch storageclass standard \\\n    -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n\nkubectl patch storageclass csi-hostpath-sc \\\n    -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre>"},{"location":"Backup%26Restore/kasten/#installing-k10","title":"Installing K10","text":"<pre><code>helm repo add kasten https://charts.kasten.io/\n</code></pre>"},{"location":"Backup%26Restore/kasten/#install-the-chart","title":"Install the chart","text":"<pre><code>helm install my-k10 kasten/k10 --version 6.0.2  --namespace=kasten-io\n</code></pre>"},{"location":"Backup%26Restore/kasten/#annotate-the-csi-hostpath-volumesnapshotclass-for-use-with-k10","title":"Annotate the CSI Hostpath VolumeSnapshotClass for use with K10","text":"<pre><code>kubectl annotate volumesnapshotclass csi-hostpath-snapclass \\\nk10.kasten.io/is-snapshot-class=true\n</code></pre>"},{"location":"Backup%26Restore/kasten/#validating-the-install","title":"Validating the Install","text":"<pre><code>kubectl get pods --namespace kasten-io --watch\n</code></pre>"},{"location":"Backup%26Restore/kasten/#validate-dashboard-access","title":"Validate Dashboard Access","text":"<pre><code>kubectl --namespace kasten-io port-forward service/gateway 8080:8000\n\nThe K10 dashboard will be available at http://127.0.0.1:8080/release-name/#/.\n\nThe K10 dashboard will be available at http://127.0.0.1:8080/k10/#/.\n</code></pre>"},{"location":"Backup%26Restore/kasten/#reference-documentation-link","title":"Reference Documentation Link:","text":"<ul> <li>https://docs.kasten.io/latest/install/other/kind.html</li> </ul> <p>For a complete list of options for accessing the Kasten K10 dashboard through a LoadBalancer, Ingress or OpenShift Route you can use the instructions</p> <ul> <li>https://docs.kasten.io/latest/access/dashboard.html#dashboard</li> </ul>"},{"location":"Logging/kafka/","title":"KAFKA","text":"<p>Introduction</p> <p>Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.</p> <p>Why Kafka? Benefits and Use Cases</p> <ul> <li>Data Integration</li> <li>Metrics and Monitoring</li> <li>Log Aggregation</li> <li>Stream Processing</li> <li>Publish-Subscribe Messaging</li> <li>Kafka Architecture \u2013 Fundamental Concepts</li> <li>Kafka Topics</li> <li>Kafka Partitioning</li> </ul> <p>How Kafka Partitioning Works</p> <p></p> <ul> <li>Kafka Brokers      Kafka brokers are like computer workers in the Kafka system. They store and handle events, manage data partitions, and work together for reliability. Think of them as the backbone of Kafka.     So, brokers are the worker bees of the Kafka system, managing events and making sure everything runs smoothly.</li> </ul> <p>Replications</p> <pre><code>- To ensure data safety, Kafka creates copies of data partitions called leader and follower replicas. When you write data, it goes to the leader, and then both leader and followers automatically replicate the data. If a node fails, another takes over. As a developer, you don't need to worry much about this process. Just know your data is secure and resilient.\nClient Applications \n- In Kafka, there are two main types of client applications: producers and consumers. Producers put messages into topics, while consumers read messages from topics. These are the building blocks for working with Kafka. Everything that's not a Kafka broker is essentially a producer or consumer, or both. They're how you connect and work with a Kafka cluster       \nKafka Producers\n- The API surface of the producer library is fairly lightweight: In Java, there is a class called KafkaProducer that you use to connect to the cluster. \nKafka Consumers\n- Using the consumer API is similar in principle to the producer. You use a class called KafkaConsumer to connect to the cluster (passing a configuration map to specify the address of the cluster, security, and other parameters).\n</code></pre> <p>Kafka Installation</p> <pre><code>Prerequisite: Java 11 or 17 should be installed on the system\nhttps://kafka.apache.org/documentation.html#java\n</code></pre> <p></p> <p>Download Apache Kafka</p> <pre><code>https://kafka.apache.org/downloads\n</code></pre> <ul> <li> <p>Extract or untar the downloaded kafka file <pre><code>tar xzvf kafka_2.12-3.5.1.tgz\n</code></pre> </p> </li> <li> <p>Now test Kafka and move the the kafka directory and use the below command    </p> </li> </ul> <pre><code>bin/kafka-topic.sh\n</code></pre> <p></p> <ul> <li>Setting the Path </li> </ul> <pre><code>Open '.bashrc' file using the command: \nvi .bashrc\n</code></pre> <p></p> <ul> <li>Move to the end and set the path using the command</li> </ul> <pre><code>export PATH=/home/rajivgs/Downloads/webi/kafka_2.12-3.5.1/bin:$PATH\n</code></pre> <p></p> <ul> <li>To test, run the command from and directory.</li> </ul> <pre><code>kafka-topic.sh\n</code></pre> <p></p> <p>If the below output is shown, it means the path is successfully set. If not, something is wrong.</p> <ul> <li>Starting Zookeeper Server on Linux</li> </ul> <pre><code>To start zookeeper, there are following below steps used\n\nStep 1: Move to the and create a new directory 'data'\n\nStep 2: Again, move to the data directory, and make two new directories as 'zookeeper' and 'kafka'.\n</code></pre> <p></p> <p>To run the zookeeper server. Open the zookeeper.properties file, which is located under the config folder.  <pre><code> vi config/zookeeper.properties\n</code></pre></p> <p>Edit the value of dataDIr by placing the path of the newly created zookeeper folder. <pre><code> dataDir=/home/rajivgs/Downloads/webi/kafka_2.12-3.5.1/data/zookeeper\n\n zookeeper-server-start.sh config/zookeeper.properties \n</code></pre></p> <p></p> <p><pre><code>zookeeper-server-start.sh config/zookeeper.properties\n</code></pre> </p> <p>Move to the 'config' folder, and open server.properties file. <pre><code>vi config/server.properties\n</code></pre></p> <pre><code>Edit the value of logs.dir=/path/data/kafka as shown below\n</code></pre> <p></p> <p>Now, run the Kafka server by using the following command: <pre><code>  kafka-server-start.sh config/server.properties\n</code></pre></p> <p></p> <p>Creating Kafka Topics Initially, make sure that both zookeeper, as well as the Kafka server, should be started.</p> <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 --create --topic topic-name  ||\nkafka-topics.sh --bootstrap-server localhost:9092 --create --topic logger --partitions 1 --replication-factor 1\n</code></pre> <p></p> <p>Here's a breakdown of the command in short points:</p> <ul> <li> <p><code>kafka-topics.sh</code>: This is the Kafka command-line tool used to manage Kafka topics.</p> </li> <li> <p><code>--bootstrap-server localhost:9092</code>: Specifies the Kafka broker(s) to connect to. In this case, it's connecting to a Kafka broker running locally on port 9092.</p> </li> <li> <p><code>--create</code>: Indicates that you want to create a new Kafka topic.</p> </li> <li> <p><code>--topic logger</code>: Specifies the name of the topic to be created, in this case, the topic name is \"logger\".</p> </li> <li> <p><code>--partitions 1</code>: Specifies the number of partitions for the topic. Partitions are the units of parallelism in Kafka. Here, you're creating the topic with 1 partition.</p> </li> <li> <p><code>--replication-factor 1</code>: Sets the replication factor for the topic. Replication ensures data durability and availability. Here, you're using a replication factor of 1, meaning there's only one copy of each partition's data.</p> </li> </ul> <p>So, the command is creating a Kafka topic named \"logger\" with a single partition and a replication factor of 1, connecting to a Kafka broker running on localhost at port 9092.</p> <p>Listing the number of Topics <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 --list\n</code></pre></p> <ul> <li> <p>Describing the topic <pre><code>kafka-topics.sh --bootstrap-server localhost:9092 -describe --topic topic-name \nkafka-topics.sh --bootstrap-server kafka2.rajivgopalsingh.com.np:9092 -describe --topic logger-01 \n</code></pre></p> </li> <li> <p>Deleting the topic <pre><code>kafka-topics.sh --bootstrap-server localhost:9092  --topic topic-name  -delete \nkafka-topics.sh --bootstrap-server kafka2.rajivgopalsingh.com.np:9092  --topic logger-01  -delete \n</code></pre></p> </li> <li> <p>Kafka Console Producer <pre><code>kafka-console-producer.sh --broker-list localhost:9092 --topic topic-name\n</code></pre></p> </li> <li> <p>Kafka Console Consumer <pre><code>kafka-console-consumerr.sh --broker-list localhost:9092 --topic topic-name  ||\nkafka-console-producer.sh --broker-list localhost:9092 --topic topic-name --from-beginning \n</code></pre></p> </li> </ul> <p>Practice:</p> <pre><code>1. https://kube-logging.dev/docs/examples/kafka-nginx/\n</code></pre>"},{"location":"Monitoring/1.%20setup_prometheus-grafana/","title":"Setup the Prometheus and Grafana","text":""},{"location":"Monitoring/1.%20setup_prometheus-grafana/#prometheus-setup","title":"Prometheus Setup","text":"<ul> <li>Prometheus Installation <pre><code> helm repo add prometheus-community https://prometheus-community.github.io/helm-charts            \n helm repo update                                                                                 \n helm install name prometheus-community/prometheus --namespace namepsace-name                     \n</code></pre></li> <li>Check the status of the pods. Check the promethues server by doing the port forward <pre><code> kubectl port-forward svc/svc_name -n namespace-name localhost-port:pod-port        \n kubectl port-forward svc/prometheus-operated -n zerone-monitoring 9090:9090                      \n</code></pre> Setup the Grafana</li> </ul> <ul> <li>Grafana Installation <pre><code> helm repo add grafana https://grafana.github.io/helm-charts                                      \n helm repo update                                                                                 \n helm install name grafana/grafana  --namespace namespace-name                                    \n</code></pre></li> <li>Get the password of the grafana <pre><code> kubectl  get secret grafana -n zerone-monitoring\n echo \u201cpassword_value\u201d  openssl base64 -d ; echo                                                 \n echo \u201cusername_value\u201d  openssl base64 -d ; echo \n kubectl port-forward svc/grafana -n zerone-monitoring 3000:3000 \n</code></pre></li> <li>Add the prometheus in the grafana dashboard. <pre><code>Prometheus server URL:  http://prometheus-operated:9090\nhttp://service-name:port\n</code></pre> </li> </ul>"},{"location":"Monitoring/1.%20setup_prometheus-grafana/#conclusion-after-setting-up-prometheus-and-grafana","title":"Conclusion After Setting Up Prometheus and Grafana","text":"<p>Setting up Prometheus and Grafana is a powerful combination for monitoring and visualizing your infrastructure and applications. After completing the setup, you can draw several conclusions and benefits:</p> <ol> <li> <p>Real-time Monitoring: With Prometheus, you have a powerful monitoring system that collects and stores metrics in real-time. This allows you to keep a close eye on the health and performance of your systems.</p> </li> <li> <p>Alerting: Prometheus provides alerting capabilities, allowing you to set up alerts based on predefined thresholds. You can be notified of issues before they become critical, helping you maintain system reliability.</p> </li> <li> <p>Historical Data: Prometheus stores historical metric data, enabling you to analyze trends and identify long-term performance patterns. This data can be invaluable for capacity planning and troubleshooting.</p> </li> <li> <p>Custom Metrics: You can instrument your applications and services to expose custom metrics, giving you deep insights into the specific aspects of your software that matter most to you.</p> </li> <li> <p>Grafana Dashboards: Grafana offers a user-friendly interface for creating and customizing dashboards that visualize Prometheus data. These dashboards can provide at-a-glance information about the state of your systems.</p> </li> <li> <p>Data Correlation: Grafana allows you to correlate data from multiple sources and display them in a single dashboard. This helps in diagnosing complex issues that involve multiple components.</p> </li> <li> <p>Flexibility: Both Prometheus and Grafana are highly customizable. You can adapt them to suit your unique monitoring needs, whether you're running a small web application or a large-scale distributed system.</p> </li> <li> <p>Community Support: Prometheus and Grafana have vibrant communities with active development and extensive documentation. You can find plugins, extensions, and support readily available online.</p> </li> <li> <p>Open Source: Both Prometheus and Grafana are open-source projects, meaning you can use them without incurring licensing costs. This makes them an economical choice for monitoring your infrastructure.</p> </li> <li> <p>Scalability: These tools are designed to scale horizontally, so you can expand your monitoring as your infrastructure grows.</p> </li> <li> <p>Integration: Prometheus and Grafana can integrate with various other tools and services, allowing you to centralize your monitoring and combine it with other aspects of your DevOps stack.</p> </li> </ol> <p>In conclusion, setting up Prometheus and Grafana provides you with a robust monitoring and visualization solution that empowers you to maintain the health and performance of your systems, make data-driven decisions, and respond proactively to issues. It's a valuable addition to any organization's infrastructure management toolkit. However, to maximize its benefits, ongoing maintenance and tuning may be required to ensure it continues to meet your evolving monitoring needs.</p>"},{"location":"Monitoring/2.%20gmail_integration/","title":"Gmail Integration","text":"<p>Description</p> <p>The application provides a Webhook integration for Prometheus AlertManager to push alerts to Google Chat rooms.</p> <ul> <li>Prometheus Installation <pre><code> helm repo add prometheus-community https://prometheus-community.github.io/helm-charts            \n helm repo update                                                                                 \n helm install name prometheus-community/prometheus --namespace namepsace-name    \n</code></pre></li> <li>Alertmanager-gchat-integration</li> </ul> <pre><code>A web application which listens for Prometheus AlertManager alerts' and forward them to Google \nChat rooms.\n</code></pre> <p><code>Install gchat manager using helm chart       helm repo add julb https://charts.julb.me   helm install name julb/alertmanager-gchat-integration  --namespace namespace-name</code></p> <ul> <li>Add the Gmail Webook in the prometheus-community-alertmanager</li> </ul> <pre><code>url : \u2018http://service.name(gchat-integration).namespace/alerts?room=lotus(roomname)\u2019\napiVersion: v1\ndata:\n  alertmanager.yml: |\n    global:\n      resolve_timeout: 1m\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'gmail'\n    receivers:\n    - name: 'gmail'\n      webhook_configs:\n      - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring/alerts?room=lotus'\n    templates:\n    - /etc/alertmanager/*.tmpl\nkind: ConfigMap\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"data\":{\"alertmanager.yml\":\"global:\\n  resolve_timeout: 1m\\nroute:\\n  group_by: ['alertname']\\n  group_wait: 10s\\n  group_interval: 10s\\n  repeat_interval: 1h\\n  receiver: 'asd'\\nreceivers:\\n- name: 'asd'\\n  webhook_configs:\\n  - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring.svc/alerts?room=lotus'\\ntemplates:\\n- /etc/alertmanager/*.tmpl\\n\"},\"kind\":\"ConfigMap\",\"metadata\":{\"annotations\":{\"meta.helm.sh/release-name\":\"prometheus-community\",\"meta.helm.sh/release-namespace\":\"zerone-monitoring\"},\"creationTimestamp\":\"2023-09-19T16:33:26Z\",\"labels\":{\"app.kubernetes.io/instance\":\"prometheus-community\",\"app.kubernetes.io/managed-by\":\"Helm\",\"app.kubernetes.io/name\":\"alertmanager\",\"app.kubernetes.io/version\":\"v0.26.0\",\"helm.sh/chart\":\"alertmanager-1.6.0\"},\"name\":\"prometheus-community-alertmanager\",\"namespace\":\"zerone-monitoring\",\"resourceVersion\":\"80702\",\"uid\":\"c171403d-1beb-44a0-a39b-b62a7472695f\"}}\n    meta.helm.sh/release-name: prometheus-community\n    meta.helm.sh/release-namespace: zerone-monitoring\n  creationTimestamp: \"2023-09-20T11:06:03Z\"\n  labels:\n    app.kubernetes.io/instance: prometheus-community\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/version: v0.26.0\n    helm.sh/chart: alertmanager-1.6.0\n  name: prometheus-community-alertmanager\n  namespace: zerone-monitoring\n  resourceVersion: \"95493\"\n  uid: c38ca1c3-0c69-48ea-8b3b-be724ab13c90\n</code></pre> <ul> <li>kubectl get cm prometheus\u2013community-server -o yaml</li> </ul> <pre><code>  alerting_rules.yml: |\n    groups:\n    - name: alertname\n      rules:\n      - alert: KubernetesNodeNotReady\n        expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n        for: 10m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes node not ready (instance {{ $labels.instance }})\n          description: \"Node {{ $labels.node }} has been unready for a long time\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesMemoryPressure\n        expr: kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes memory pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has MemoryPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesDiskPressure\n        expr: kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes disk pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has DiskPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesNetworkUnavailable\n        expr: kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes network unavailable (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has NetworkUnavailable condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesOutOfCapacity\n        expr: sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 &gt; 90\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes out of capacity (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} is out of capacity\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesContainerOomKiller\n        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes container oom killer (instance {{ $labels.instance }})\n          description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPersistentvolumeclaimPending\n        expr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n          description: \"PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPodCrashLooping\n        expr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})\n          description: \"Pod {{ $labels.pod }} is crash looping\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: ImagePullBackoffAlert\n        expr: kube_pod_container_status_waiting_reason{reason=\"ImagePullBackOff\"} == 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is in ImagePullBackOff state\"\n          description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing to pull its container image.\"\n</code></pre> <ul> <li>Get the google chat space url </li> </ul> <p></p> <p></p> <ul> <li> <p>Encode  the following to base64. <pre><code>[app.notification]\n# Jinja2 custom template to print message to GChat.\ncustom_template_path = \"/opt/alertmanager-gchat-integration/cm/notification-template-json.j2\"\n[app.room.lotus(google_space_name)]\nnotification_url = \u2018&lt;google_chat_space_url&gt;\u2019\n</code></pre></p> </li> <li> <p>Use the convert base64 code in the Secret (gchat-integration) &amp;&amp; paste  in config.toml section <pre><code>apiVersion: v1\ndata:\n  config.toml: &lt;Encoded base64 value&gt;&gt;\nkind: Secret\n</code></pre></p> </li> <li> <p>After updating the Secret and ConfigMap we need to restart some Pod:  <pre><code>    - alert-manager-alertmanager-gchat-integration\n    - prometheus-community-alertmanager\n    - prometheus-community-server\n</code></pre></p> </li> </ul> <p>Output</p> <p></p>"},{"location":"Monitoring/3.%20slack_integration/","title":"Slack Integration","text":"<p>Setup the Prometheus and Grafana</p> <p>Generating the Webhook of the Slack.</p> <p>Go to app and create a new slack notifications webhooks.</p> <p> </p> <ul> <li> <p>kubectl get cm prometheus-community-alertmanager -o yaml <pre><code> alertmanager.yml: |\n    global:\n      resolve_timeout: 1m\n      slack_api_url: 'https://hooks.slack.com/services/T023XD85BFA/B05XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG00vrsFjos50iid'\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'slack-notifications'\n    receivers:\n    - name: 'slack-notifications'\n      slack_configs:\n      - channel: '#&lt;add here the name of the slack channel&gt;'\n        send_resolved: true\n        icon_url: https://avatars3.githubusercontent.com/u/3380462\n        icon_emoji: ':fire:'\n        title: |-\n          [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}\n          {{- if gt (len .CommonLabels) (len .GroupLabels) -}}\n            {{\" \"}}(\n            {{- with .CommonLabels.Remove .GroupLabels.Names }}\n              {{- range $index, $label := .SortedPairs -}}\n                {{ if $index }}, {{ end }}\n                {{- $label.Name }}=\"{{ $label.Value -}}\"\n              {{- end }}\n            {{- end -}}\n            )\n          {{- end }}\n        text: |-\n          {{ range .Alerts -}} *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}\n          *Description:* {{ .Annotations.description }}\n          *Details:*\n            {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}`\n            {{ end }}\n          {{ end }}\n    templates:\n    - /etc/alertmanager/*.tmpl\n</code></pre></p> </li> <li> <p>kubectl get cm prometheus\u2013community-server -o yaml</p> </li> </ul> <pre><code>  alerting_rules.yml: |\n    groups:\n    - name: alertname\n      rules:\n      - alert: KubernetesNodeNotReady\n        expr: kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 0\n        for: 10m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes node not ready (instance {{ $labels.instance }})\n          description: \"Node {{ $labels.node }} has been unready for a long time\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesMemoryPressure\n        expr: kube_node_status_condition{condition=\"MemoryPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes memory pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has MemoryPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesDiskPressure\n        expr: kube_node_status_condition{condition=\"DiskPressure\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes disk pressure (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has DiskPressure condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesNetworkUnavailable\n        expr: kube_node_status_condition{condition=\"NetworkUnavailable\",status=\"true\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes network unavailable (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} has NetworkUnavailable condition\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesOutOfCapacity\n        expr: sum by (node) ((kube_pod_status_phase{phase=\"Running\"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\"\"})) / sum by (node) (kube_node_status_allocatable{resource=\"pods\"}) * 100 &gt; 90\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes out of capacity (instance {{ $labels.instance }})\n          description: \"{{ $labels.node }} is out of capacity\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesContainerOomKiller\n        expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m &gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[10m]) == 1\n        for: 0m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes container oom killer (instance {{ $labels.instance }})\n          description: \"Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPersistentvolumeclaimPending\n        expr: kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})\n          description: \"PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: KubernetesPodCrashLooping\n        expr: increase(kube_pod_container_status_restarts_total[1m]) &gt; 3\n        for: 2m\n        labels:\n          severity: critical\n        annotations:\n          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})\n          description: \"Pod {{ $labels.pod }} is crash looping\\n  VALUE = {{ $value }}\\n  LABELS = {{ $labels }}\"\n      - alert: ImagePullBackoffAlert\n        expr: kube_pod_container_status_waiting_reason{reason=\"ImagePullBackOff\"} == 1\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is in ImagePullBackOff state\"\n          description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is failing to pull its container image.\"\n</code></pre> <ul> <li>After Completing the above steps it shows alerts like below example</li> </ul> <p></p> <ul> <li>For both Gmail and Slack <pre><code>apiVersion: v1\ndata:\n  alertmanager.yml: |-\n    global:\n      resolve_timeout: 1m\n      slack_api_url: 'https://hooks.slack.com/services/T023XD85BFA/B05XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXG00vrsFjos50iid'\n    route:\n      group_by: ['alertname']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 1h\n      receiver: 'slack-notifications'\n      routes:\n        # Route for Gmail notifications\n        - receiver: 'gmail-notification'\n          match:\n            severity: 'critical'  # Define the condition to route to Gmail\n          continue: true  # Continue processing other routes\n        # Route for Slack notifications\n        - receiver: 'slack-notifications'\n          match:\n             severity: 'critical'\n          continue: true\n    receivers:\n    - name: 'slack-notifications'\n      slack_configs:\n      - channel: '#lotus'\n        send_resolved: true\n        icon_url: https://avatars3.githubusercontent.com/u/3380462\n        icon_emoji: ':fire:'\n        title: |-\n          [{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }} for {{ .CommonLabels.job }}\n          {{- if gt (len .CommonLabels) (len .GroupLabels) -}}\n            {{ \" \" }}(\n            {{- with .CommonLabels.Remove .GroupLabels.Names }}\n              {{- range $index, $label := .SortedPairs -}}\n                {{ if $index }}, {{ end }}\n                {{- $label.Name }}=\"{{ $label.Value -}}\"\n              {{- end }}\n            {{- end -}}\n            )\n          {{- end }}\n        text: |-\n          {{ range .Alerts -}} *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}\n          *Description:* {{ .Annotations.description }}\n          *Details:*\n            {{ range .Labels.SortedPairs }} \u2022 *{{ .Name }}:* `{{ .Value }}`\n            {{ end }}\n          {{ end }}\n    - name: 'gmail-notification'\n      webhook_configs:\n      - url: 'http://alert-manager-alertmanager-gchat-integration.zerone-monitoring.svc/alerts?room=lotus'\n    templates:\n    - /etc/alertmanager/*.tmpl\nkind: ConfigMap\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: prometheus-community\n    meta.helm.sh/release-namespace: zerone-monitoring\n  creationTimestamp: \"2023-09-19T16:33:26Z\"\n  labels:\n    app.kubernetes.io/instance: prometheus-community\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/version: v0.26.0\n    helm.sh/chart: alertmanager-1.6.0\n  name: prometheus-community-alertmanager\n  namespace: zerone-monitoring\n  resourceVersion: \"80702\"\n  uid: c171403d-1beb-44a0-a39b-b62a7472695f\n</code></pre></li> </ul> <p>References <pre><code>https://artifacthub.io/packages/helm/prometheus-community/prometheus\nhttps://medium.com/globant/setup-prometheus-and-grafana-monitoring-on-kubernetes-cluster-using-helm-3484efd85891\nhttps://grafana.com/blog/2020/02/25/step-by-step-guide-to-setting-up-prometheus-alertmanager-with-slack-pagerduty-and-gmail/\nhttps://samber.github.io/awesome-prometheus-alerts/rules#kubernetes\n</code></pre></p>"},{"location":"MultiCluster/Karmada/","title":"Prerequisites","text":""},{"location":"MultiCluster/Karmada/#mkcert","title":"mkcert","text":"<p>Installation: </p> <pre><code> https://github.com/FiloSottile/mkcert\n</code></pre> <pre><code>  mkcert clustermanager.local \"*.clustermanager.local\"\n\n  # Create TLS secret from generated certificates\n\n  kubectl create secret tls tls-secret \\\n      --cert=clustermanager.com+1.pem \\\n      --key=clustermanager.com+1-key.pem \\\n      -n default --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"MultiCluster/Karmada/#metallb-local-development","title":"MetalLB  (Local development)","text":"<pre><code>  echo \"Installing metalLB\"\n  kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.5/config/manifests/metallb-native.yaml\n  sleep 5\n  while true\n  do\n      ready=`kubectl get pod -n metallb-system -l component=controller | grep controller | awk '{print $2}'`\n      if [ \"$ready\" == \"1/1\" ]\n      then\n          break\n      fi\n      echo \"Metallb status :: $ready, sleeping for 10 seconds...\"\n      sleep 10\n      done\n  network=$(docker network inspect -f '{{.IPAM.Config}}' kind | awk '{print $1}' | cut -f 1,2 -d '.' | cut -f 1 -d '{' --complement)\n  if [ -z \"$network\" ]\n  then\n      network=\"172.18\"\n  fi\n  printf \"apiVersion: metallb.io/v1beta1\\nkind: IPAddressPool\\nmetadata:\\n  name: example\\n  namespace: metallb-system\\nspec:\\n  addresses:\\n  - $network.254.200-$network.254.250\\n---\\napiVersion: metallb.io/v1beta1\\nkind: L2Advertisement\\nmetadata:\\n  name: empty\\n  namespace: metallb-system\" | kubectl apply -f -\n  sleep 10\n</code></pre>"},{"location":"MultiCluster/Karmada/#ingress-nginx","title":"Ingress-Nginx","text":"<pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm install ingress-nginx ingress-nginx/ingress-nginx -n ingress-nginx --create-namespace --version 4.12.1\n</code></pre>"},{"location":"MultiCluster/Karmada/#certificate-manager","title":"Certificate Manager","text":"<p>Before installing the chart, you must first install the cert-manager CustomResourceDefinition resources. This is performed in a separate step to allow you to easily uninstall and reinstall cert-manager without deleting your installed custom resources.</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.17.1/cert-manager.crds.yaml\n\nhelm repo add cert-manager https://charts.jetstack.io\nhelm install cert-manager cert-manager/cert-manager -n cert-manager --create-namespace --version 1.17.1\n</code></pre>"},{"location":"MultiCluster/Karmada/#install-karmada-control-plane","title":"Install Karmada Control Plane","text":"<p>Github : https://github.com/karmada-io/karmada</p> <p><pre><code>helm repo add karmada-charts https://raw.githubusercontent.com/karmada-io/karmada/master/charts\nhelm install karmada -n karmada-system --create-namespace --dependency-update ./charts/karmada\n</code></pre> - values.yaml <pre><code>certs:\n  ## @param certs.mode \"auto\" and \"custom\" are provided\n  ## \"auto\" means auto generate certificate\n  ## \"custom\" means use user certificate\n  mode: auto\n  auto:\n    ## @param certs.auto.expiry expiry of the certificate\n    expiry: 43800h\n    ## @param certs.auto.hosts hosts of the certificate\n    hosts: [\n      \"kubernetes.default.svc\",\n      \"*.etcd.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}\",\n      \"*.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}\",\n      \"*.{{ .Release.Namespace }}.svc\",\n      \"localhost\",\n      \"*.clustermanager.com\",\n      \"127.0.0.1\"\n    ]\n</code></pre> - Need to provide the host for the karmada-apiserver ingress to be publicly accessible.</p> <pre><code> This will install Karmada core components like Karmada API server, scheduler, controller-manager, etc., in the karmada-system namespace.\n</code></pre>"},{"location":"MultiCluster/Karmada/#create-the-karmada-apiserver-ingress","title":"Create the karmada-apiserver Ingress:","text":"<p><pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\n  name: karmada\n  namespace: karmada-system\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: karmada.clustermanager.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: karmada-apiserver\n            port:\n              number: 5443\n        path: /\n        pathType: ImplementationSpecific\n  tls:\n  - hosts:\n    - karmada.clustermanager.com\n    secretName: tls-secret\n</code></pre> - &gt; Note (in case of any issues): <pre><code>    nginx.ingress.kubernetes.io/upstream-vhost: karmada-apiserver.karmada-system.svc\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/proxy-buffer-size: 8k\n    nginx.ingress.kubernetes.io/proxy-buffers-number: \"4\"\n    nginx.ingress.kubernetes.io/proxy-ssl-verify: \"off\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/secure-backends: \"true\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n</code></pre></p>"},{"location":"MultiCluster/Karmada/#during-the-karmada-helm-chart-deployment-a-secret-containing-the-karmada-apiserver-kubeconfig-is-created-extract-this-kubeconfig-file-which-is-used-to-configure-the-karmada-apiserver-context","title":"During the Karmada Helm chart deployment, a secret containing the karmada-apiserver kubeconfig is created. Extract this kubeconfig file, which is used to configure the karmada-apiserver context.","text":""},{"location":"MultiCluster/Karmada/#extract-the-base64-encoded-kubeconfig","title":"Extract the Base64 Encoded Kubeconfig:","text":"<ul> <li> <p>There are several ways to achieve this:</p> </li> <li> <p>Using kubectl and yq (recommended if you have yq installed) : <pre><code>kubectl get secret karmada-kubeconfig -n karmada-system -o jsonpath='{.data.kubeconfig}' | base64 -d &gt; karmada.config\n</code></pre></p> </li> </ul> <p>Explanation :</p> <ul> <li>kubectl get secret karmada-kubeconfig -n karmada-system -o jsonpath='{.data.kubeconfig}' : This extracts the value of the kubeconfig key from the data field of the secret, which is the base64 encoded kubeconfig.</li> <li>base64 -d : This command decodes the base64 string.</li> <li>karmada.config : This redirects the decoded output to a file named karmada.config.</li> </ul> <p>Using kubectl and jq (if you have jq installed) :</p> <pre><code>kubectl get secret karmada-kubeconfig -n karmada-system -o json | jq -r '.data.kubeconfig' | base64 -d &gt; karmada.config\n</code></pre> <p>Explanation :</p> <ul> <li>kubectl get secret karmada-kubeconfig -n karmada-system -o json :  Retrieves the secret in JSON format. </li> <li>jq -r '.data.kubeconfig' : Parses the JSON and extracts the kubeconfig value (raw, without - quotes).</li> <li>base64 -d : Decodes the base64 string.</li> <li>karmada.config : Redirects the decoded output to the karmada.config file.</li> </ul> <p>Using kubectl, sed, and awk (more manual but works without extra tools) : <pre><code>kubectl get secret karmada-kubeconfig -n karmada-system -o yaml | sed -n '/kubeconfig:/,$p' | awk '{if(NR&gt;1)print}' | tr -d ' ' | base64 -d &gt; karmada.config\n</code></pre></p> <p>Explanation :</p> <ul> <li>kubectl get secret karmada-kubeconfig -n karmada-system -o yaml : Gets the secret in YAML format.</li> <li>sed -n '/kubeconfig:/,$p' : Extracts lines from kubeconfig: to the end of the file.</li> <li>awk '{if(NR&gt;1)print}' : Skips the first line (which is kubeconfig:)</li> <li>tr -d ' ': Removes leading spaces.</li> <li>base64 -d : Decodes the base64 string.</li> <li>karmada.config : Saves the decoded content to karmada.config.</li> </ul>"},{"location":"MultiCluster/Karmada/#verify-the-file-optional","title":"Verify the File (Optional):","text":"<ul> <li>You can verify that the karmada.config file is a valid kubeconfig file by using the kubectl command with the --kubeconfig flag: <pre><code>kubectl --kubeconfig=karmada.config config view\n</code></pre></li> </ul> <p>Important Notes:</p> <ul> <li>base64 -d : The base64 -d command might be slightly different depending on your operating system. On macOS, you might need to use base64 -D.</li> <li>yq and jq : These are powerful command-line tools for processing YAML and JSON data. If you don't have them, you can usually install them with your system's package manager (e.g., apt-get install yq jq, brew install yq jq).</li> <li>Security: Be mindful of where you store your karmada.config file, as it contains credentials to access your Karmada control plane.</li> </ul> <p>Inside the karamada.config file, you can find the following information: <pre><code>apiVersion: v1\nclusters:\n- cluster: \n    certificate-authority-data: xxx\n  server:  https://karmada-apiserver.karmada-system.svc.cluster.local:5443\ncontexts:\n- context:\n    cluster: karmada-apiserver\n    user: karmada-apiserver\n  name: karmada-apiserver\ncurrent-context: karmada\nkind: Config\npreferences: {}\nusers:\n- name: karmada-apiserver\n  user: \n    client-certifiticate-date: xxx\n    client-key-data: xxx\n    name: karmada-apiserver\n</code></pre></p> <p>Here, we the need to use the endpoints in the kubeconfig file to access the Karmada API server.   i.e \"https://karmada.clustermanager.com\" in place of \"https://karmada-apiserver.karmada-system.svc.cluster.local:5443\"</p> <p>Verify karmada-apiserver after the server is updated:</p> <pre><code>export KUBECONFIG=karmada.config\nkubectl config get-contexts\n</code></pre>"},{"location":"MultiCluster/Karmada/#install-karmada-agent","title":"Install Karmada Agent","text":"<pre><code>helm install karmada karmada/karmada --namespace karmada-system --create-namespace -f values.yaml\n</code></pre> <ul> <li>values.yaml</li> </ul> <pre><code>installMode: \"agent\"\nagent:\n  clusterName: \"kind-whitelotus\"   # Name of the member cluster\n  clusterEndpoint: \"https://127.0.0.1:46339\" # Member cluster endpoint\n  ## kubeconfig of the karmada-apiserver \n  kubeconfig:\n    caCrt: |                      # Certificate Authority Data\n      -----BEGIN CERTIFICATE-----\n      xxx\n      -----END CERTIFICATE-----\n    crt: |                        # Client Certificate Data\n      -----BEGIN CERTIFICATE-----\n      xxx\n      -----END CERTIFICATE-----\n    key: |                        # Client Key Data\n      -----BEGIN RSA PRIVATE KEY-----\n      xxx\n      -----END RSA PRIVATE KEY-----\n    server: \"https://karmada.clustermanager.com\"\n</code></pre> <ul> <li>This installs the Karmada agent on the Kind cluster, allowing it to receive workloads scheduled by Karmada. </li> </ul> <p>Verify in Karmada host cluster <pre><code>export KUBECONFIG=karmada.config\nkubectl config get-contexts \nkubectl get clusters\n</code></pre> You should see \"kind-whitelotus\" listed as a member cluster with status \"Ready\".</p> <ul> <li>Verify Karmada mode:Agent <pre><code>export KUBECONFIG=whitelotus.config\nkubectl get pods -n karmada-system  \n</code></pre></li> </ul> <p>Create the Propagation Policy <pre><code>apiVersion: policy.karmada.io/v1alpha1\nkind: PropagationPolicy\nmetadata:\n  name: nginx-policy\nspec:\n  resourceSelectors:\n  - apiVersion: apps/v1\n    kind: Deployment\n    name: nginx-deployment-23\n  placement:\n    clusterAffinity:\n      clusterNames:\n        - kind-member3\n</code></pre></p> <p>Create a workload in the whitelotus cluster</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment-23\n  namespace: default\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"Security/KubeScape/","title":"KubeScape","text":"<p>Kubescape is an open source Kubernetes security platform for your IDE, CI/CD pipelines, and clusters. Kubescape includes risk analysis, security compliance, and misconfiguration scanning.</p>"},{"location":"Security/KubeScape/#installing-kubescape","title":"Installing Kubescape","text":"<p><pre><code>curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash \n</code></pre> </p> <p>Run a scan <pre><code>kubescape scan --enable-host-scan --verbose\n\n\u2018kubescape scan\u2019 : command to scan k8s clusters\n\u2018--enable-host-scan\u2019:  scan not just the Kubernetes cluster itself, but also the underlying hosts on which the cluster is running.\n\u2018--verbose\u2019: to provide additional information during the scan.\n</code></pre></p> <p></p> <pre><code>        Figure: Scanning the k8s cluster for Security risk and Vulnerabilities\n</code></pre> <ul> <li> <p>Get results!</p> </li> </ul> <p></p> <pre><code>        Figure: List of Security risk and Vulnerabilities\n</code></pre>"},{"location":"Security/KubeScape/#armosec-with-kubescape","title":"ArmoSec with KubeScape","text":"<ul> <li> <p>Naviagte to cloud.armosec.io/account</p> </li> </ul> <p></p> <ul> <li> <p>Connect the K8s Cluster</p> </li> </ul> <p></p> <p></p> <ul> <li> <p>Check the compliance</p> </li> </ul> <p></p> <ul> <li> <p>Check Dashboard for k8s cluster</p> </li> </ul> <p></p>"},{"location":"Security/KubeScape/#architecture-of-kubescape","title":"Architecture of Kubescape","text":"<p>The main components of the Kubescape architecture are:</p> <ul> <li> <p>Scanner:</p> <p>This is the core component of Kubescape. It scans the Kubernetes cluster and identifies potential security risks and vulnerabilities. It uses different techniques to analyze the Kubernetes configuration and resources, such as identifying misconfigured network policies, insecure container images, and non-compliant Kubernetes objects.</p> </li> <li> <p>Policies: </p> <p>Kubescape uses a set of predefined policies to evaluate the Kubernetes environment. These policies are customizable and can be modified to suit the specific security requirements of an organization. The policies are written in YAML and can be easily extended to include additional checks.</p> </li> <li> <p>Result: </p> <p>The results component of Kubescape provides a detailed report of the security risks and vulnerabilities identified during the scan. It categorizes the findings based on their severity and provides remediation steps for each issue.</p> </li> <li> <p>Web interface: </p> <p>Kubescape comes with a web interface that allows users to interact with the tool and view the scan results. The interface provides an easy-to-use dashboard that summarizes the scan findings and allows users to drill down into specific issues.</p> </li> <li> <p>API:</p> <p>Kubescape also exposes an API that can be used to integrate with other security tools and automate the scanning process. The API can be used to trigger scans, retrieve scan results, and perform other actions programmatically.</p> </li> </ul>"},{"location":"Security/KubeScape/#usage-of-kubescape","title":"Usage of Kubescape","text":"<ul> <li> <p>Security auditing: </p> <p>Kubescape can be used to audit the security of a Kubernetes environment by scanning for security risks and vulnerabilities. It can help security teams to identify potential security issues and take the necessary steps to remediate them.</p> </li> <li> <p>Compliance testing: </p> <p>Kubescape can also be used to test Kubernetes clusters against compliance standards such as PCI-DSS, HIPAA, or SOC 2. It can help organizations to ensure that their Kubernetes environments are compliant with industry standards and regulations.</p> </li> <li> <p>DevOps integration: </p> <p>Kubescape can be integrated into the DevOps pipeline to perform security checks automatically. This can help organizations to ensure that their Kubernetes deployments are secure and compliant before they are deployed.</p> </li> <li> <p>Continuous Monitoring: </p> <p>Kubescape can be used to continuously monitor the security of a Kubernetes environment by running scheduled scans. This can help organizations to detect and remediate security risks and vulnerabilities in a timely manner.</p> </li> </ul>"},{"location":"Security/KubeScape/#demonstration","title":"Demonstration","text":"<p>Installation:</p> <p><pre><code>curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash \n</code></pre> </p> <pre><code>kubescape scan  --enable-host-scan\n</code></pre> <p> </p> <pre><code>    Scan the whole cluster in the system\n</code></pre> <ul> <li> <p>Create and Deploy any Kubernetes Deployment file in the cluster</p> </li> </ul> <p></p> <ul> <li> <p>Scan the created deployment file <pre><code>kubescape scan filename\n</code></pre></p> </li> </ul> <p></p> <pre><code>    Scanning the deployment file with vulnerabilities\n</code></pre> <ul> <li> <p>Solving the Resources Limit vulnerabilities</p> </li> </ul> <p></p> <pre><code>    Adding the limit and request resources inside the deployment yaml file\n</code></pre> <ul> <li> <p>Resources Limit Vulnerabilities problem solved</p> </li> </ul> <p></p> <p>Figure: Resources Limit Vulnerabilities Solved</p> <ul> <li> <p>Uploading the severity in the json format <pre><code>kubescape scan nginx-deploy.yaml --format json --output op.json\n</code></pre> </p> <pre><code>Saved the op.json file\n</code></pre> </li> </ul>"},{"location":"Security/M9Sweeper/","title":"M9Sweeper","text":""},{"location":"Security/M9Sweeper/#introduction","title":"Introduction","text":"<p>M9sweeper is a free and easy kubernetes security platform.    It integrates industry standard open source utilities into a one-stop-shop kubernetes security tool that can walk most kubernetes administrators through securing a kubernetes cluster as well as the apps running on the cluster.</p>"},{"location":"Security/M9Sweeper/#feature","title":"Feature","text":"<p>M9sweeper makes securing a cluster easy with:</p> <ul> <li>CVE Scanning</li> <li>Enforcement of CVE Scanning Rules</li> <li>Reports and Dashboards</li> <li>CIS Security Benchmarking</li> <li>Pen Testing</li> <li>Deployment Coaching</li> <li>Intrusion Detection</li> <li>Gatekeeper Policy Management</li> </ul>"},{"location":"Security/M9Sweeper/#prerequisites","title":"\u203c\ufe0f Prerequisites","text":"<ul> <li>This project requires a Kubernetes Cluster and uses helm as package manager  Mac Install from terminal</li> </ul>"},{"location":"Security/M9Sweeper/#for-intel-macs","title":"For Intel Macs","text":"<pre><code>[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-amd64\n</code></pre>"},{"location":"Security/M9Sweeper/#for-m1-arm-macs","title":"For M1 / ARM Macs","text":"<pre><code>[ $(uname -m) = arm64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-darwin-arm64\nchmod +x ./kind\nmv ./kind /some-dir-in-your-PATH/kind\n</code></pre>"},{"location":"Security/M9Sweeper/#windows","title":"Windows","text":"<pre><code>Install with chocolatey and install a bash client\n\nchoco install kind\nchoco install git\n</code></pre>"},{"location":"Security/M9Sweeper/#start-kubernetes","title":"Start Kubernetes","text":"<pre><code>kind create cluster --name cluster_name\n</code></pre> <p>Quick Installation for the M9Sweeper</p> <pre><code>helm repo add m9sweeper https://m9sweeper.github.io/m9sweeper \nhelm repo update \n helm install  m9sweeper m9sweeper/m9sweeper --install --wait \\\n  --create-namespace --namespace m9sweeper-system \\\n  --set-string dash.init.superAdminEmail=\"email\" \\\n  --set-string dash.init.superAdminPassword=\"password\" \\\n  --set-string global.jwtSecret=\"changeme\" \\\n  --set-string global.apiKey=\"ffb40975b2694226\"\n</code></pre>"},{"location":"Security/M9Sweeper/#cve-scanning","title":"CVE Scanning","text":"<ul> <li>Common Vulnerability Exposures/Enumeration gives common names to openly known security issues or vulnerabilities. </li> <li>The objective of CVE is to make it simpler to impart information over different databases and make available a common platform to evaluate security tools.</li> </ul>"},{"location":"Security/M9Sweeper/#trivy","title":"Trivy","text":"<ul> <li>Trivy is a comprehensive and versatile security scanner. </li> <li>Trivy has scanners that look for security issues, and targets where it can find those issues.</li> </ul> <p>Targets (what Trivy can scan):</p> <ul> <li>Container Image</li> <li>Filesystem</li> <li>Git Repository (remote)</li> <li>Virtual Machine Image</li> <li>Kubernetes</li> <li>AWS</li> </ul> <p>Scanners (what Trivy can find there):</p> <ul> <li>OS packages and software dependencies in use (SBOM)</li> <li>Known vulnerabilities (CVEs)</li> <li>IaC issues and misconfigurations</li> <li>Sensitive information and secrets</li> <li>Software licences</li> </ul> <p>How CVE Scanning works in M9Sweeper</p> <ul> <li>Login into the M9Sweeper</li> <li>After Logged in the below dashboard is visible  Fig: M9Sweeper Dashboard</li> <li>Choose the cluster for security checkup   Fig: Default cluster already present in the dashboard</li> <li>After choosing the cluster navigate into the Images  in the dashboard  Fig: List of Images present in the dashboard  Fig: We can manually add and scan the images using the URL  Fig: Output of the manual scan image </li> </ul> <p>Here Can check the severity level and CVE Types explaining about the CVE Vulnerabilities Weakness,Affected Software,Potential Mitigations and References with score level. See the details of the Compliant report  Apply for Request Exception</p> <ul> <li> <p>Enforcement of CVE Scanning Rules     Enforcement of Common Vulnerabilities and Exposures (CVE) scanning rules is a critical aspect of maintaining the security of computer systems and networks.   Reports and Dashboards, including historical reporting to see how your security posture has changed over time  Fig: Report summary on the dashboard</p> </li> <li> <p>CIS Security Benchmarking (Center for Internet Security)</p> </li> <li>Kube-bench    kube-bench is a tool that checks whether Kubernetes is deployed securely by running the checks documented in the CIS Kubernetes Benchmark.</li> </ul> <p>In the context of kube-bench, \"Standard Kubernetes Environment - Just Master Benchmarks\" refers to a specific configuration or set of security benchmarks that are focused on the control plane components (master components) of a Kubernetes cluster. </p> <p> Fig: Selecting the k8s env for benchmark  Fig: Installation of kube-bench   Fig: Installation Output for the benchmark   Fig: Output</p> <p>Pen Testing       Assess whether you are secure by running a regular, scheduled non-invasive penetration test using kube-hunter. Results can be reviewed in the user interface.   Kube-hunter   kube-hunter hunts for security weaknesses in Kubernetes clusters. The tool was developed to increase awareness and visibility for security issues in Kubernetes environments.</p> <p> Fig: Installation command   Fig: Installation of kube-hunter   Fig: Waiting for the result   Fig: Result Output </p> <p>Deployment Coachings   Ensure deployments are properly secured by reviewing your application deployments with kubesec. Ratings and summary reports are available in a simple graphical user interface.  Fig: Choose the pods from cluster or file   Fig: Reports generate after scanning the cluster</p> <p>Intrusion Detection   Monitor applications for suspicious activity and/or intrusions by hackers (such as reverse shells) using Project Falco. Detect anomalies by receiving alerts whenever a new kind of suspicious behaviour is detected.</p> <p>Project Falco     Falco is a cloud native runtime security tool for Linux operating systems. It is designed to detect and alert on abnormal behavior and potential security threats in real-time.</p> <p>At its core, Falco is a kernel monitoring and detection agent that observes events, such as syscalls, based on custom rules. Falco can enhance these events by integrating metadata from the container runtime and Kubernetes. The collected events can be analyzed off-host in SIEM or data lake systems.  Fig: Installation of Falco</p>"},{"location":"Security/M9Sweeper/#gatekeeper-policy-management","title":"Gatekeeper Policy Management","text":"<p>Gatekeeper is an admission controller that validates requests to create and update Pods on Kubernetes clusters, using the Open Policy Agent (OPA).    Using Gatekeeper allows administrators to define policies with a constraint, which is a set of conditions that permit or deny deployment behaviors in Kubernetes.   Policy Controller for Kubernetes.</p> <ul> <li>Installation   Fig: Install the Gatekeeper  Note (Gatekeeper for workloads else internal server error)  Fig: copy the command given in the clipboard and paste in the terminal   Fig:GateKeeper Installed in the Cluster </li> </ul>"},{"location":"Traefik/Traefik/","title":"Traefik","text":""},{"location":"Traefik/Traefik/#introduction","title":"Introduction","text":"<p>Traefik is based on the concept of EntryPoints, Routers, Middlewares and Services.</p> <p>The main features include dynamic configuration, automatic service discovery, and support for multiple backends and protocols.</p> <p>EntryPoints: EntryPoints are the network entry points into Traefik. They define the port which will receive the packets, and whether to listen for TCP or UDP.</p> <p>Routers: A router is in charge of connecting incoming requests to the services that can handle them.</p> <p>Middlewares: Attached to the routers, middlewares can modify the requests or responses before they are sent to your service</p> <p>Services: Services are responsible for configuring how to reach the actual services that will eventually handle the incoming requests.</p> <p>Applied some simple postgres manifest for the TCP routing </p> <p></p> <p>Dashboard Image with the entrypoints </p> <p></p> <p>Exposing the Entrypoints using the postgres </p> <p></p>"},{"location":"Traefik/Traefik/#installation","title":"Installation","text":"<pre><code>helm repo add traefik https://helm.traefik.io/traefik\nhelm repo update\nhelm install traefik traefik/traefik --version 28.3.0 -f values.yaml\n\nadditionalArguments: \n  - \"--entryPoints.postgres.address=:5432\"\n  - \"--entrypoints.redis.address=:6379\"\n  - \"--entrypoints.mariadb.address=:3306\"\n  - \"--certificatesresolvers.le-staging.acme.tlschallenge=true\"\n  - \"--certificatesresolvers.le-staging.acme.email=xxx@gmail.com\"\n  - \"--certificatesresolvers.le-staging.acme.storage=/data/acme.json\"\n  - \"--certificatesresolvers.le-staging.acme.caserver=https://acme-staging-v02.api.letsencrypt.org/directory\"\n #  - \"--providers.kubernetesingress.ingressclass=traefik-internal\"\n  - \"--log.level=DEBUG\"\nports:\n  redis:\n    port: 6379\n    protocol: TCP\n    expose: \n      default: true\n    tls:\n      enabled: true    \n  postgres:\n    expose:\n      default: true\n    exposePort: 5432\n    port: 5432\n    protocol: TCP\n    transport:\n      respondingTimeouts:\n        readTimeout: 0\n        writeTimeout: 0\n        idleTimeout: 0\n      keepAliveMaxRequests: 0\n      keepAliveMaxTime: 0\n  mariadb:\n    expose:\n      default: true\n    exposePort: 3306\n    port: 3306\n    protocol: TCP\n    transport:\n      respondingTimeouts:\n        readTimeout: 0\n        writeTimeout: 0\n        idleTimeout: 0\n      keepAliveMaxRequests: 0\n      keepAliveMaxTime: 0\n</code></pre>"},{"location":"Traefik/Traefik/#redis","title":"Redis","text":"<pre><code>Installation\n\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-redis bitnami/redis --version 19.5.5\n</code></pre> <ul> <li> <p>Manifest the IngressRouteTCP for Redis  <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: redis-service-tcp-clone-1\n  namespace: redis\nspec:\n  entryPoints:\n    - redis\n  routes:\n  - match: HostSNI(`redis1.db.rajivgopalsingh.com.np`)\n    services:\n    - name: redis-master\n      namespace: redis\n      port: 6379\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre></p> </li> <li> <p>Test the Redis server using the CLI  <pre><code>redis-cli -h redis1.db.rajivgopalsingh.com.np -p 6379 -a qk50H1nOdY --tls --sni redis1.db.rajivgopalsingh.com.np\n</code></pre> Note:  <pre><code>when changing the values of the tls.passthrough: true occurs\n Error: Protocol error, got \"H\" as reply type byte\n</code></pre></p> </li> </ul>"},{"location":"Traefik/Traefik/#postgresql","title":"PostgreSQL","text":"<p>Tested with the Crunchy Postgres and CloudNative PG  CrunchyData PG</p> <pre><code>Installation:\nkubectl create -f https://operatorhub.io/install/postgresql.yaml\n</code></pre> <p>CloudNative PG </p> <pre><code>Installation:\nkubectl create -f https://operatorhub.io/install/cloudnative-pg.yaml\n</code></pre> <p>Bitnami PG     helm repo add bitnami https://charts.bitnami.com/bitnami     helm install my-postgresql bitnami/postgresql --version 15.5.7</p> <p>IngressRouteTCP manifest for the PG <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: deployment-625-postgres-ha\n  namespace: env-203\nspec:\n  entryPoints:\n    - postgres\n  routes:\n  - match: HostSNI(`*`)\n    services:\n    - name: deployment-625-postgres-ha\n      namespace: env-203\n      port: 5432\n  tls:\n    certResolver: le-staging\n    domains:                   \n    - main: db.rajivgopalsingh.com.np        \n      sans:                    \n      - postgres625.db.rajivgopalsingh.com.np\n      - post625.db.rajivgopalsingh.com.np\n    passthrough: true \n---\napiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: deployment-626-postgres-ha\n  namespace: env-203\nspec:\n  entryPoints:\n    - postgres\n  routes:\n  - match: HostSNI(`postgres626.db.rajivgopalsingh.com.np`)\n    services:\n    - name: deployment-626-postgres-ha\n      namespace: env-203\n      port: 5432\n  tls:\n    certResolver: le-staging\n    passthrough: true \n</code></pre> Output</p> <p></p>"},{"location":"Traefik/Traefik/#mariadb","title":"MariaDB","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-mariadb bitnami/mariadb --version 18.2.4\n</code></pre> <p>Manifest IngressRouteTCP for the MariaDB</p> <pre><code> apiVersion: traefik.io/v1alpha1\n kind: IngressRouteTCP\n metadata:\n   name: deployment-628-mariadb\n   namespace: env-203\n spec:\n   entryPoints:\n     - mariadb\n   routes:\n   - match: HostSNI(`*`)\n     services:\n     - name: deployment-628-mariadb\n       namespace: env-203\n       port: 3306\n</code></pre> <pre><code>Currently while setuping the MariaDB the above manifest is working only if we are providing the  wildcard in the HOSTSNI\n</code></pre> <p>Output</p> <p></p> <p>Mongodb     helm repo add bitnami https://charts.bitnami.com/bitnami     helm install my-mongodb bitnami/mongodb --version 15.6.11</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: mongodb\nspec:\n  entryPoints:\n    - mongodb\n  routes:\n    - match: HostSNI(`mongodb.db.umeshkhatiwada.com.np`)\n    priority: 10\n    services:\n        - name: my-mongodb\n        port: 27017\n        weight: 10\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre> <p>Cmd: mongo --host host1.com --port 27017 -u root -p cRIt05TMRr --authenticationDatabase admin --ssl --sslAllowInvalidCertificates</p>"},{"location":"Traefik/Traefik/#rabbitmq","title":"Rabbitmq","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install my-rabbitmq bitnami/rabbitmq --version 14.4.4\n</code></pre> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRouteTCP\nmetadata:\n  name: rabbitmq\nspec:\n  entryPoints:\n    - amqp\n  routes:\n    - match: HostSNI(`rabbitmq.db.umeshkhatiwada.com.np`)\n    priority: 10\n    services:\n        - name: my-rabbitmq\n        port: 5672\n        weight: 10\n  tls:\n    certResolver: le-staging\n    passthrough: false\n</code></pre> <p>Testscript: <pre><code>import pika\nimport ssl\nimport socket\n\n# RabbitMQ server configuration\nhostname = 'host1.com'  # Replace with your RabbitMQ server hostname or IP address\nport = 5672  # Default RabbitMQ SSL port\nusername = 'user'  # RabbitMQ username\npassword = 'BE9zlp2MQugpQVbY'  # RabbitMQ password\nvirtual_host = '/'  # RabbitMQ virtual host\n\n# SSL/TLS configuration\ncontext = ssl.create_default_context()\n\n# Disable hostname checking and certificate verification for self-signed certificates\ncontext.check_hostname = False\ncontext.verify_mode = ssl.CERT_NONE\n\n# TCP Keep-Alive settings\nsocket.setdefaulttimeout(10)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 1)\nsocket.socket(socket.AF_INET, socket.SOCK_STREAM).setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 5)\n\n# Connect to RabbitMQ server with SSL\ncredentials = pika.PlainCredentials(username, password)\nparameters = pika.ConnectionParameters(\n    host=hostname,\n    port=port,\n    virtual_host=virtual_host,\n    credentials=credentials,\n    ssl_options=pika.SSLOptions(context)\n)\ntry:\n    connection = pika.BlockingConnection(parameters)\n    channel = connection.channel()\n\n    # Declare a queue\n    queue_name = 'hello'\n    channel.queue_declare(queue=queue_name)\n\n    # Publish a message to the queue\n    message = 'Hello, RabbitMQ with SSL!'\n    channel.basic_publish(exchange='', routing_key=queue_name, body=message)\n    print(f\" [x] Sent '{message}'\")\n\n    # Close connection\n    connection.close()\nexcept Exception as e:\n    print(f\"Failed to connect to RabbitMQ server: {e}\")\n</code></pre></p>"},{"location":"Traefik/Traefik/#reference","title":"Reference","text":"<ul> <li> <p>Is it possible to use Traefik to proxy PostgreSQL over SSL?</p> </li> <li> <p>\ud83d\udd28 Support Traefik v3 features \u00b7 Issue #757 \u00b7 traefik/traefik-helm-chart </p> </li> <li> <p>How to connect to Traefik TCP Services with TLS configuration enabled? </p> </li> <li> <p>Release v3.0.0-rc5 \u00b7 traefik/traefik </p> </li> <li> <p>Using Traefik as Ingress on different MetalLB loadbalancerIP? </p> </li> <li> <p>Exposing a postgres service through Traefik IngressRouteTCP and Cloudflare DNS </p> </li> </ul>"},{"location":"Traefik/Traefik/#workshop","title":"Workshop","text":"<ul> <li> <p>GitHub - traefik-workshops/traefik-workshop</p> <p>Getting started with Traefik on Kubernetes. A few exercises that help you to start the journey with Traefik Proxy. </p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/","title":"How to rotate and renew expired Contour secrets (contourcert and envoycert)","text":"<p>The purpose  is to provide a detailed guide on how to rotate and renew expired secrets (contourcert and envoycert) under projectcountour namespace. It includes a step-by-step workaround, prerequisites, and verification steps.</p>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#symptoms","title":"Symptoms:","text":"<ul> <li> <p>Envoy (pods with name starting with \"projectcontour-envoy-\") pods under projectcontour namespace show restarts.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#example","title":"Example","text":"<pre><code>kubectl get pods -n proejctcontour\n\nNAMESPACE        NAME                                     READY  STATUS    RESTARTS  AGE  \nprojectcontour   projectcontour-contour-c476b465b-b5bvn    1/1   Running    1        16h  \nprojectcontour   projectcontour-contour-c476b465b-jkhsd    1/1   Running    1        16h                    \nprojectcontour   projectcontour-envoy-wegsv                1/2   Running    161      16h          \nprojectcontour   projectcontour-envoy-sd7xc                1/2   Running    161      16h      \nprojectcontour   projectcontour-envoy-u9hac                1/2   Running    162      16h\n</code></pre> <ul> <li> <p>The secrets (contourcert and envoycert) in projectcontour namespace are expired.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#example_1","title":"Example","text":"<pre><code>kubectl  get secret -n projectcontour envoycert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\nnotBefore=Jul 24 19:11:31 2023 GMT\nnotAfter=Jul 26 19:11:31 2024 GMT\n\nkubectl  get secret -n projectcontour contourcert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\nnotBefore=Jul 24 19:11:31 2023 GMT\nnotAfter=Jul 26 19:11:31 2024 GMT\n</code></pre> <ul> <li> <p>The log for envoy pods in the projectcontour namespace (pods with name starting with \"projectcontour-envoy-\") reports TLS errors, such as: <pre><code>  kubectl  logs -n projectcontour &lt;envoy pod name&gt;\n</code></pre></p> </li> <li> <p>[2024-07-26 16:32:01.575][1][warning][config] [bazel-out/k8-opt/bin/source/common/config/_virtual_includes/grpc_stream_lib/common/config/grpc_stream.h:101] StreamListeners gRPC config stream closed: 14, upstream connect error or disconnect/reset before headers. reset reason: connection failure, transport failure reason: TLS error: 268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#cause","title":"Cause","text":"<ul> <li> <p>The secrets (contourcert and envoycert) are used for internal gRPC communication between Contour and Envoy.</p> </li> </ul>"},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#update-the-contour-and-under-all-the-resources-defined-in-contour_certgen_jobyaml-which-has-following-tags","title":"Update the contour  and  under all the resources defined in \"contour_certgen_job.yaml\" which has following tags: <pre><code>\"helm.sh/chart: contour-&lt;version&gt;\"\n\"image: docker.io/bitnami/contour:&lt;image_version&gt;\"\nYou can get the current version by running \"kubectl describe\" on contour deployment/pods.\n</code></pre> <p>Run the following commands from manager node to rotate/renew the expired contourcert/envoycert (you can run these commands on the control plane node, there is no need to pass kubeconfig option in that case):</p> <pre><code>1. Take backup:\n    kubectl get secret -n projectcontour contourcert -o yaml &gt; contourcert-backup.yaml\n  kubectl get secret -n projectcontour envoycert -o yaml &gt; envoycert-backup.yaml\n\n2. Delete current contourcert and envoycert:\n   kubectl delete secret -n projectcontour envoycert contourcert\n\n3. Generate new contourcert and envoycert :\n    kubectl apply -f &lt;path&gt;/contour_certgen_job.yaml -n projectcontour\n\n4. Verify that the new contourcert/envoycert was generated:\n    kubectl get secret -n projectcontour\n\n5. Verify that the validity period of new secrets is 10 years:\n  kubectl get secret -n projectcontour envoycert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\n  kubectl get secret -n projectcontour contourcert -o jsonpath='{.data.ca\\.crt}' | base64 -d | openssl x509 -noout -dates\n\n6. Restart all contour pods:\n   kubectl patch deployment projectcontour-contour -n projectcontour -p '{\"spec\": {\"template\": {\"metadata\": {\"labels\":{\"test\": \"restart\"} } } } }' --type=merge\n\n7. Restart all envoy pods:\n  kubectl patch daemonset projectcontour-envoy -n projectcontour -p '{\"spec\": {\"template\": {\"metadata\": {\"labels\":{\"test\": \"restart\"} } } } }' --type=merge\n\n8. Delete projectcontour-contour-certgen job if it is present:\n  kubectl get job projectcontour-contour-certgen -n projectcontour\n\n    If the output of above command shows the projectcontour-contour-certgen job, then run below command to delete it: \n  kubectl delete job projectcontour-contour-certgen -n projectcontour\n\n9. Verify all contour/envoy pods are in running state and not restarting:\n  kubectl get pods -n projectcontour\n</code></pre>","text":""},{"location":"Troubleshoots/How%20to%20rotate%20and%20renew%20expired%20Contou/#manifest","title":"Manifest <pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\nspec:\n  allowPrivilegeEscalation: true\n  allowedCapabilities:\n  - '*'\n  fsGroup:\n    rule: RunAsAny\n  hostIPC: true\n  hostNetwork: true\n  hostPID: true\n  hostPorts:\n  - max: 65535\n    min: 0\n  privileged: true\n  runAsUser:\n    rule: RunAsAny\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n  volumes:\n  - '*'\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - projectcontour-contour-certgen-psp\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: projectcontour-contour-certgen-psp\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: projectcontour-contour-certgen-psp\nsubjects:\n  - kind: ServiceAccount\n    name: projectcontour-contour-certgen\n    namespace: zerone-projectcontour\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - secrets\n    verbs:\n      - create\n      - update\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: projectcontour-contour-certgen\nsubjects:\n  - kind: ServiceAccount\n    name: projectcontour-contour-certgen\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: projectcontour-contour-certgen\n  namespace: zerone-projectcontour\n  annotations:\n    \"helm.sh/hook\": \"pre-install,pre-upgrade\"\n    \"helm.sh/hook-weight\": \"1\"\n    \"helm.sh/hook-delete-policy\": before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: contour\n    helm.sh/chart: contour-10.2.2\n    app.kubernetes.io/instance: projectcontour\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: contour-certgen\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: contour\n        helm.sh/chart: contour-10.2.2\n        app.kubernetes.io/instance: projectcontour\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: contour-certgen\n    spec:\n\n      containers:\n        - name: contour\n          image: docker.io/bitnami/contour:1.23.3-debian-11-r0\n          imagePullPolicy: IfNotPresent\n          command:\n            - contour\n          args:\n            - certgen\n            - --kube\n            - --incluster\n            - --overwrite\n            - --secrets-format=compact\n            - --namespace=$(CONTOUR_NAMESPACE)\n            - --certificate-lifetime=3650\n          env:\n            - name: CONTOUR_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n          resources:\n            limits:\n              memory: 256Mi\n            requests:\n              cpu: 40m\n              memory: 32Mi\n      restartPolicy: Never\n      serviceAccountName: projectcontour-contour-certgen\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        fsGroup:\n        runAsNonRoot: true\n  parallelism: 1\n  completions: 1\n  backoffLimit: 1\n</code></pre>","text":""},{"location":"assets/Docker-Image-Security/","title":"Docker Image Security","text":"<p>Automated Docker Image Security with Kaniko, Cosign, and Kyverno</p> <p>Figure: Secure Docker Image Lifecycle Management Diagram Flow This image shows a workflow diagram for building, signing, and deploying a Docker container in a Kubernetes environment. Here's a summary of the key steps and components: The process starts with a Dockerfile, used to build a Docker image using Kaniko. The built Docker image is then signed using Cosign and a private key. The signed image is verified using Kyverno, public key, and cluster policy. After verification, a pod is created in Kubernetes. The diagram also shows integration with CI/CD tools and container registries: GitLab container registry is used in the process of the workflow Key security components include: Private and public keys for signing and verification Cosign for image signing Kyverno for policy enforcement and signature verification This workflow represents a secure container deployment process, emphasizing image signing, verification, and policy enforcement before deployment to a Kubernetes cluster. Kaniko Build Docker Image  Kaniko, developed by Google, is a popular tool in the software development world. Kaniko is designed to build container images from a Dockerfile, inside a container or Kubernetes cluster, without needing Docker daemon access. This makes creating images in environments like Kubernetes safer, especially when considering the security risks associated with Docker daemon\u2019s privileged mode.</p> <p>It can build accurate Docker images just like Docker would.  It does this by executing each command in the Dockerfile in userspace, making a new layer in the filesystem for each command. </p> <p>So, using Kaniko in your Docker image-building workflow can make it more accurate, secure, and efficient. It\u2019s also simple to include in your GitLab CI/CD pipeline. Now, let\u2019s set up the build stage using Kaniko in GitLab CI.</p> <p>Note:  Understand the Difference: Docker:  Docker uses a daemon-based approach where the Docker daemon runs on the host machine and builds the image. This requires privileged access, which can be a security concern, especially in Kubernetes clusters. Docker Daemon is a persistent background process that manages Docker images, containers, networks, and storage volumes. Kaniko:  Kaniko is a tool to build container images from a Dockerfile, inside a container or Kubernetes cluster. It doesn\u2019t require special privileges, making it more secure for Kubernetes environments. Now, let\u2019s set up the build stage using Kaniko in GitLab CI. Required CI/CD variables for the workflow </p> <p>stages:   - build-and-push</p> <p>build-and-push:   stage: build-and-push   image:     name: gcr.io/kaniko-project/executor:v1.14.0-debug     entrypoint: [\"\"]   only:     - main   script:     - echo \"Authenticating to Docker Hub...\"     - AUTH=(echo -n \"DOCKERHUB_USER:DOCKERHUB_TOKEN\" | base64)     - echo \"{\\\"auths\\\":{\\\"(echo DOCKERHUB_AUTH_URL | sed 's/\"/\\\\\"/g')\\\":{\\\"auth\\\":\\\"(echo AUTH | sed 's/\"/\\\\\"/g')\\\"}}}\" &gt; /kaniko/.docker/config.json     - echo \"Authentication file created. Contents (redacted):\"     - cat /kaniko/.docker/config.json | sed 's/\"auth\":\"[^\"]*\"/\"auth\":\"REDACTED\"/g'     - echo \"Running Kaniko executor...\"     - /kaniko/executor        --build-arg IMAGE_NAME=IMAGE_NAME       --build-arg IMAGE_TAG=IMAGE_TAG       --destination docker.io/DOCKERHUB_REPO/{DOCKERHUB_IMAGE_NAME}-develop:{CI_COMMIT_TAG}       --digest-file=/tmp/digest       --verbosity debug     - echo \"IMAGE_REF=docker.io/DOCKERHUB_REPO/{DOCKERHUB_IMAGE_NAME}-develop:{CI_COMMIT_TAG}\" &gt;&gt; build.env     - echo \"IMAGE_DIGEST=(cat /tmp/digest)\" &gt;&gt; build.env   artifacts:     reports:       dotenv: build.env     paths:       - build.env</p> <p>Cosign</p> <p>A concise summary of how Cosign can be used to sign and verify Docker images: Cosign: Securing the Software Supply Chain Key Features: Open-source tool to sign, verify, and store containers in OCI registries Part of the Sigstore project to improve software supply chain security Confirms Docker image origin and integrity to ensure safe image use Easily integrates into existing workflows with minimal changes Useful for automating signing in CI/CD pipelines Getting Started with Cosign: Create a public/private key pair using the cosign generate-key-pair command Store the keys securely in GitLab environment variables or a secrets manager Use Cosign to sign Docker images before pushing them to a registry Verify signed images before pulling and using them Keyless Signing with Sigstore: Cosign supports keyless signing via the Sigstore project Eliminates the need to manage and rotate private signing keys Cosign requests a short-lived key pair, records it in a transparency log, then discards it The key is generated using an OIDC token from the CI/CD pipeline Cosign provides a straightforward way to add cryptographic signing and verification to your Docker image workflow, improving the security of your software supply chain.</p> <p>Now, with the keys created and stored safely, we can set up a GitLab CI job to use Cosign for image signing.</p> <p>stages:   - sign</p> <p>sign:   stage: sign   image: docker:latest   needs:     - build-and-push   services:     - docker:dind   variables:     COSIGN_YES: \"true\"   id_tokens:     SIGSTORE_ID_TOKEN:       aud: sigstore   script:     - apk add --no-cache cosign     - echo \"Logging in to Docker Hub...\"     - echo \"DOCKERHUB_TOKEN\" | docker login -u \"DOCKERHUB_USER\" --password-stdin \"DOCKERHUB_AUTH_URL\"     - echo \"Signing the image...\"     - IMAGE_REF=(grep IMAGE_REF build.env | cut -d'=' -f2)     - IMAGE_DIGEST=(grep IMAGE_DIGEST build.env | cut -d'=' -f2)     - cosign sign \"{IMAGE_REF}@{IMAGE_DIGEzST}\"     - echo \"Image signed successfully\"     - docker logout \"DOCKERHUB_AUTH_URL\"     - echo \"Verifying the signed image...\"     - cosign verify \"{IMAGE_REF}@{IMAGE_DIGEST}\" --certificate-identity \"CI_PROJECT_URL//.gitlab-ci.yml@refs/heads/CI_COMMIT_BRANCH\" --certificate-oidc-issuer \"$CI_SERVER_URL\"     - echo \"Image verified successfully\"   artifacts:     paths:       - build.env</p> <p>Container Signing + Admission controller This repository shows an example of how to use GitLab CI to sign container images with cosign. When new commits are pushed to the main, this project will build a new container image and sign it using cosign. The Kubernetes admission controller can then be used to prevent unsigned images from running in your Kubernetes cluster. To set up the admission controller for this purpose:</p> <p>policy.yaml apiVersion: policy.sigstore.dev/v1beta1 kind: ClusterImagePolicy metadata:   name: image-policy spec:   images:     - glob: \"**\"   authorities:     - keyless:         url: \"https://fulcio.sigstore.dev\"         identities:           - issuer: https://gitlab.com             subject: https://gitlab.com/bwill/container-signing//.gitlab-ci.yml@refs/heads/main</p> <p>Image Verification using cosign  IMAGE_REF=\"index.docker.io/rajivgs/node-develop\" IMAGE_DIGEST=\"sha256:b3c2acbad551a2793c0d97073c654a560fb272c8e5310f31c27ea35b4c246f7b\" CI_PROJECT_URL=\"https://gitlab.com/RajivGS/KCK\" CI_COMMIT_BRANCH=\"main\" CI_SERVER_URL=\"https://gitlab.com\"</p> <p>cosign verify \"{IMAGE_REF}@{IMAGE_DIGEST}\" \\   --certificate-identity \"CI_PROJECT_URL//.gitlab-ci.yml@refs/heads/CI_COMMIT_BRANCH\" \\   --certificate-oidc-issuer \"$CI_SERVER_URL\"</p> <p>Note: Signing Image with Sigstore: Before signing any image, we need to publish the image to a registry. Cosign does not support the signing of images that have not been published to a registry. On top of that, you need to have write permission to that registry.</p> <p>As you can see, essentially, the signature is stored within the same image registry as the main image. The signature carries the identical image digest, with the addition of \u201c.sig\u201d appended to it. Very easy to remember! Up to this moment, we uploaded the image and the image signature. What\u2019s next? Now let\u2019s verify it in Kubernetes using Kyverno!</p> <p>Kyverno Install Kyverno using Helm: Kyverno can be deployed via a Helm chart\u2013the recommended and preferred method for a production install\u2013which is accessible either through the Kyverno repository or on Artifact Hub. Both generally available and pre-releases are available with Helm. Full documentation at: https://kyverno.io Note: Require kubeVersion &gt;=1.25 for kyverno  To install Kyverno with Helm, first add the Kyverno Helm repository.</p> <p>helm repo add kyverno https://kyverno.github.io/kyverno/ helm install kyverno --namespace kyverno kyverno/kyverno --create-namespace</p> <p>To secure our Kubernetes clusters, it is essential to ensure that only trusted, signed container images are deployed. Kyverno is a policy engine for Kubernetes that can be used to enforce this security policy.</p> <p>To use Kyverno to verify container image signatures, we can define a ClusterPolicy CR that specifies the container images to be verified and the required digests. This policy will prevent unauthorized images from being deployed to the cluster.</p> <p>Here are the steps involved:</p> <p>Define the specific container image to be verified, either by its tag or SHA. Create a Kyverno ClusterPolicy CR that specifies the container image to be verified and the required digest. Apply the ClusterPolicy CR to the Kubernetes cluster. Once the ClusterPolicy CR is applied, Kyverno will intercept all requests to create new pods and deployments. If the container image specified in the request does not have a valid signature or does not match the required digest, Kyverno will deny the request.</p> <p>ClusterPolicy </p> <p>apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata:   name: verify-node-develop-image-signature spec:  #Specifies that if the image signature verification fails, the Pod will be rejected.   validationFailureAction: Enforce</p>"},{"location":"assets/Docker-Image-Security/#indicates-that-the-policy-will-be-applied-during-admission-control","title":"Indicates that the policy will be applied during admission control,","text":"<p>background: false    rules:</p>"},{"location":"assets/Docker-Image-Security/#specifies-that-this-rule-applies-to-pods","title":"Specifies that this rule applies to Pods.","text":"<pre><code>- name: verify-image-signature   \nmatch:\n    resources:\n      kinds:\n        - Pod\n  verifyImages:\n  - image: \"index.docker.io/rajivgs/node-develop:*\"\n</code></pre>"},{"location":"assets/Docker-Image-Security/#enables-verification-of-the-image-digest-sha256-hash","title":"Enables verification of the image digest (SHA256 hash).","text":"<pre><code>    verifyDigest: true\n</code></pre>"},{"location":"assets/Docker-Image-Security/#enables-mutating-the-pod-specification-to-include-the-verified-image-digest","title":"Enables mutating the Pod specification to include the verified image digest.","text":"<pre><code>    mutateDigest: true\n</code></pre> <p># verifying the image signature. attestors:          - entries:</p>"},{"location":"assets/Docker-Image-Security/#specifies-a-keyless-attestor","title":"Specifies a keyless attestor.","text":"<pre><code>      - keyless:\n</code></pre> <p>#Specifies the subject of the attestation, which is the GitLab CI/CD pipeline configuration file at the main branch.               subject:  \"https://gitlab.com/RajivGS/KCK//.gitlab-ci.yml@refs/heads/main\"                    issuer: \"https://gitlab.com\"</p>"},{"location":"assets/Docker-Image-Security/#rekor-transparency-log-server-that-is-used-to-verify-the-attestation-of-the-docker-image-signature","title":"Rekor transparency log server that is used to verify the attestation of the Docker image signature.","text":"<pre><code>          rekor:\n            url: \"https://rekor.sigstore.dev\"\n</code></pre> <p>Note:  When applying the Kyverno ClusterPolicy manifest \"verify-node-develop-image-signature\" to a Kubernetes cluster, it will be enforced on any Pod that uses the \"rajivgs/node-develop\" Docker image, regardless of whether the image is specified directly in the Pod manifest or in a higher-level resource like a Deployment.</p> <p>Create a manifest and use the unsigned docker image </p> <p>apiVersion: v1 kind: Pod metadata:   name: node-develop-fail spec:   containers:   - name: node-develop     image: index.docker.io/rajivgs/node-develop:unsigned</p> <p>Now, we deploy the  clusterpolicy, docker signed  image, and unsigned image we can set up a GitLab CI job to use</p> <p>deploy:   stage: deploy   image:      name: bitnami/kubectl:latest     entrypoint: [\"\"]   needs:     - sign   before_script:     - echo \"Kubernetes config file path KUBECONFIG\"     - export KUBECONFIG=KUBECONFIG      - kubectl config view   script:     - echo \"Deploying signed image to Kubernetes cluster...\"     - IMAGE_REF=(grep IMAGE_REF build.env | cut -d'=' -f2)     - IMAGE_DIGEST=(grep IMAGE_DIGEST build.env | cut -d'=' -f2)     - FULL_IMAGE_REF=\"{IMAGE_REF}@{IMAGE_DIGEST}\"     # Create Kyverno ClusterPolicy manifest     - |       cat &lt;&lt; EOF &gt; kyverno-policy.yaml       apiVersion: kyverno.io/v1       kind: ClusterPolicy       metadata:         name: verify-node-develop-image-signature       spec:         validationFailureAction: Enforce         background: false         rules:           - name: verify-image-signature             match:               resources:                 kinds:                   - Pod             verifyImages:             - image: \"index.docker.io/rajivgs/node-develop:*\"               verifyDigest: true               mutateDigest: true               attestors:               - entries:                 - keyless:                     subject: \"https://gitlab.com/RajivGS/KCK//.gitlab-ci.yml@refs/heads/main\"                     issuer: \"https://gitlab.com\"                     rekor:                       url: \"https://rekor.sigstore.dev\"       EOF     # Apply Kyverno ClusterPolicy     - echo \"Applying Kyverno ClusterPolicy...\"     - kubectl apply -f kyverno-policy.yaml     # Create a temporary deployment YAML     - |       cat &lt;&lt; EOF &gt; deployment.yaml       apiVersion: apps/v1       kind: Deployment       metadata:         name: ${DOCKERHUB_IMAGE_NAME}-deployment       spec:         replicas: 1         selector:           matchLabels:             app: ${DOCKERHUB_IMAGE_NAME}         template:           metadata:             labels:               app: ${DOCKERHUB_IMAGE_NAME}           spec:             containers:             - name: ${DOCKERHUB_IMAGE_NAME}               image: ${FULL_IMAGE_REF}               imagePullPolicy: Always       EOF     # Apply the deployment     - kubectl apply -f deployment.yaml  --validate=false     # Create a manifest file for reference     - |       cat &lt;&lt; EOF &gt; manifest.yaml       apiVersion: apps/v1       kind: Deployment       metadata:         name: ${DOCKERHUB_IMAGE_NAME}-deployment       spec:         template:           spec:             containers:             - name: ${DOCKERHUB_IMAGE_NAME}               image: ${FULL_IMAGE_REF}       EOF     # Create unsigned image deployment     - |       cat &lt;&lt; EOF &gt; unsigned-deployment.yaml       apiVersion: v1       kind: Pod       metadata:         name: node-develop-fail       spec:         containers:         - name: node-develop           image: index.docker.io/rajivgs/node-develop:unsigned       EOF     # Attempt to apply unsigned image deployment (expected to fail)     - echo \"Attempting to apply unsigned deployment (expected to fail)...\"     - kubectl apply -f unsigned-deployment.yaml --validate=false || true     - echo \"Deployments created and applied\"     - echo \"Deployment created and manifest generated\"   artifacts:     paths:       - deployment.yaml       - manifest.yaml       - unsigned-deployment.yaml</p> <p>Reference: Url: https://github.com/GoogleContainerTools/kaniko https://www.youtube.com/watch?v=EgwVQN6GNJg&amp;t=976s https://docs.sigstore.dev/signing/quickstart/ https://docs.gitlab.com/ee/ci/ https://gitlab.com/bwill/container-signing/-/tree/aeb9ba494f2b5fb761c91ca53a221295eaf79ef3 https://kyverno.io/ https://github.com/vfarcic/kaniko-demo https://medium.com/@glen.yu/why-i-prefer-kyverno-over-gatekeeper-for-native-kubernetes-policy-management-35a05bb94964</p> <p>Queries: Why using Kaniko instead of the Docker </p> <p>Figure: Kaniko </p> <p>Figure: Docker </p> <p>When getting started, you\u2019re likely to run into an error like this: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</p> <p>securityContext:   privileged: true   allowPrivilegeEscalation: true</p> <p>Caching is challenging in Docker, as changes to any layer will invalidate the entire cache for that layer and subsequent layers. Kaniko provides an efficient cache management system where only the necessary layers are invalidated, leading to better cache utilization and faster builds. Comparison between Docker and Kaniko</p> <p>Aspect Docker Kaniko Build Process Monolithic, performed inside Docker daemon Distributed, each step executed in separate container Privileged Access Requires privileged access to Docker daemon Doesn't require privileged access Build Context Entire project directory sent to Docker daemon Users can define specific files/directories Rebuild Trigger Changes in project directory trigger complete rebuild Only rebuilds necessary parts that have changed Build Performance Can be slower due to intermediate containers and layers Faster due to incremental build process Build Environment Tightly coupled to host machine More isolated and reproducible Compatibility May encounter issues when moving between environments Consistent builds across different platforms Cache Management Maintains cache of intermediate layers More efficient cache management system Cache Invalidation Changes invalidate cache for that layer and subsequent ones Only necessary layers are invalidated Security Potential risks due to privileged daemon access Additional security from isolated build steps Attack Surface Larger due to daemon access requirement Reduced, eliminating need for privileged access</p> <p>Cosign alternatives and similar packages Spire. 8.3 9.7 Cosign VS Spire. The SPIFFE Runtime Environment. in-toto. 5.1 8.3 Cosign VS in-toto. A Go implementation of in-toto. ... Spiffe-Vault. 3.6 8.3 Cosign VS Spiffe-Vault. Integrates Spiffe and Vault to have secretless authentication.</p> <p>How does Kaniko enhance security in the build environment? Kaniko eliminates the need for a Docker daemon, reducing potential security risks in the build environment by executing each command in userspace, creating a new filesystem layer for each command.</p> <p>What is the primary function of Cosign in this pipeline? Cosign helps ensure the integrity and provenance of Docker images by signing them, verifying their source, and preventing tampering. It integrates well with existing workflows and can be automated as part of a CI/CD pipeline.</p> <p>How does Kyverno enforce image signing policies? Kyverno is a Kubernetes policy management tool that helps enforce best practices within the cluster. It can verify image signatures, ensuring only properly signed images are used. Kyverno policies can be defined to verify image signatures and enforce the use of signed images.</p> <p>What are the benefits of this automated pipeline? The automated pipeline offers improved security, enhanced efficiency, and increased consistency. It ensures that only signed and verified images are deployed, reducing the risk of deploying tampered or unauthorized images.</p> <p>How can you monitor the health and status of the automated processes? You can monitor the health and status of the automated processes using tools that provide visibility and reporting on the security posture of your Docker images across the organization. This includes monitoring the build, sign, and verify stages to ensure they are running smoothly and efficiently.</p> <p>How does this pipeline integrate with your existing DevOps toolchain and CI/CD workflows? The pipeline integrates seamlessly with GitLab CI/CD pipelines, allowing for the automation of the build, sign, and verify process. This integration ensures that the pipeline is part of your broader DevOps workflow and can be managed and updated centrally.</p> <p>Scalability and Performance Kaniko can efficiently build Docker images within Kubernetes clusters, replicating the functionality of Docker in a secure and scalable manner. Cosign integrates well with existing workflows and can be automated as part of a CI/CD pipeline, allowing it to handle the volume of image signings required. Kyverno is a Kubernetes-native policy management tool that can scale with your infrastructure, enforcing image signing policies consistently across all environments</p> <p>Reliability and Fault Tolerance Implement robust error handling and retries in your pipeline scripts to ensure resilience to temporary failures. Monitor the health and status of the automated processes using tools like Prometheus and Grafana, which can provide visibility into the performance and reliability of the pipeline. Set up alerting mechanisms to quickly notify the team of any issues or failures, allowing for prompt investigation and resolution. Consistency and Governance Define your image signing policies as Kyverno ClusterPolicies, which can be consistently applied across all namespaces and clusters. Use tools like Argo CD to manage and synchronize the Kyverno policies across your environments, ensuring they are up-to-date and compliant with your security standards. Regularly review and update the policies to adapt to changing security requirements and best practices. Integration and Visibility Integrate the pipeline with your existing GitLab CI/CD workflows by defining multiple build stages using Kaniko, allowing for flexibility and customization. Use tools like Grafana to gain comprehensive visibility into the security posture of your Docker images across the organization, including build, sign, and verify stages. Generate reports on image vulnerabilities, policy violations, and signing status to maintain oversight and compliance.</p>"},{"location":"assets/keda/","title":"Keda","text":"<p>KEDA Introduction      KEDA is a single-purpose and lightweight component that can be added into any Kubernetes cluster. How KEDA Work  KEDA performs three key roles within Kubernetes: Agent \u2014 KEDA activates and deactivates Kubernetes Deployments to scale to and from zero on no events. This is one of the primary roles of the keda-operator container that runs when you install KEDA. Metrics \u2014 KEDA acts as a Kubernetes metrics server that exposes rich event data like queue length or stream lag to the Horizontal Pod Autoscaler to drive scale out. It is up to the Deployment to consume the events directly from the source. This preserves rich event integration and enables gestures like completing or abandoning queue messages to work out of the box. The metric serving is the primary role of the keda-operator-metrics-apiserver container that runs when you install KEDA. Admission Webhooks - Automatically validate resource changes to prevent misconfiguration and enforce best practices by using an admission controller. As an example, it will prevent multiple ScaledObjects to target the same scale target.</p> <p>Architecture   The diagram below shows how KEDA works in conjunction with the Kubernetes Horizontal Pod Autoscaler, external event sources, and Kubernetes\u2019 etcd data store</p> <p>Event sources and scalers KEDA has a wide range of scalers that can both detect if a deployment should be activated or deactivated, and feed custom metrics for a specific event source. The following scalers are available:</p> <p>Custom Resources (CRD) When you install KEDA, it creates four custom resources: scaledobjects.keda.sh scaledjobs.keda.sh triggerauthentications.keda.sh clustertriggerauthentications.keda.sh These custom resources enable you to map an event source (and the authentication to that event source) to a Deployment, StatefulSet, Custom Resource or Job for scaling. ScaledObjects represent the desired mapping between an event source (e.g. Rabbit MQ) and the Kubernetes Deployment, StatefulSet or any Custom Resource that defines /scale subresource. ScaledJobs represent the mapping between event source and Kubernetes Job. ScaledObject/ScaledJob may also reference a TriggerAuthentication or ClusterTriggerAuthentication which contains the authentication configuration or secrets to monitor the event source.</p> <p>Deploy KEDA     Installation </p> <p>Deploying with Helm    helm repo add kedacore https://kedacore.github.io/charts  helm install keda kedacore/keda -n keda --create-namespace </p> <p>Deploying with Operator Hub curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.27.0/install.sh | bash -s v0.27.0</p> <p>kubectl create -f https://operatorhub.io/install/keda.yaml</p> <p>YAML declarations</p>"},{"location":"assets/keda/#including-admission-webhooks","title":"Including admission webhooks","text":"<p>kubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.1/keda-2.13.1.yaml</p>"},{"location":"assets/keda/#without-admission-webhooks","title":"Without admission webhooks","text":"<p>kubectl apply --server-side -f https://github.com/kedacore/keda/releases/download/v2.13.1/keda-2.13.1-core.yaml</p> <p>Keda Concept</p> <p>Scaling Deployments, StatefulSets &amp; Custom Resources Scaling Jobs Authentication External Scalers Admission Webhooks Troubleshooting</p> <p>Scaling Deployments, StatefulSets &amp; Custom Resources Manifest </p> <p>apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata:  name: scaledobj-wp-1  namespace:     zerone-1-1 spec:  scaleTargetRef:    apiVersion:    apps/v1    kind:          Deployment    name:          zerone-metadata-1-wordpress  pollingInterval:  30  cooldownPeriod:   300  idleReplicaCount: 0  minReplicaCount:  1  maxReplicaCount:  15  fallback:    failureThreshold: 3    replicas: 6  advanced:    restoreToOriginalReplicaCount: false    horizontalPodAutoscalerConfig:      behavior:        scaleDown:          stabilizationWindowSeconds: 300          policies:          - type: Percent            value: 55            periodSeconds: 15  triggers:  - type: memory    metricType: Utilization # Allowed types are 'Utilization' or 'AverageValue'    metadata:      value: \"30\"      containerName: \"wordpress\"</p>"}]}